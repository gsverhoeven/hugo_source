---
title: "Optimal performance with Random Forests: does feature selection beat tuning?"
author: "Gertjan Verhoeven"
date: '2022-01-03'
summary: This blog post demonstrates that the presence of irrelevant variables can reduce the performance of the Random Forest algorithm (as implemented in R by `ranger()`). The solution is either to tune one of the algorithm's parameters, OR to remove irrelevant features using a procedure called Recursive Feature Elimination (RFE). 
slug: random-forest-overfitting
draft: TRUE
categories: 
  - Random Forest
  - Machine Learning
  - Overfitting
tags:
  - caret
  - tidymodels
  - ranger
baseurl: https://gsverhoeven.github.io
header:
  image: headers/wilhelm-gunkel-di8ognBauG0-unsplash.png
  preview: no
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(AppliedPredictiveModeling)
library(caret)
library(tidyverse)
library(mlbench)
library(ranger)
```

Models using decision trees have an interesting property, they perform "automatic" feature (predictor / variable) selection, as it is part of the model building process. Contrast this with linear regression: such a model is forced to use all the features provided to it. What happens if we add irrelevant features, i.e. features that are unrelated to the outcome we want to predict? We expect linear regression to suffer from it, because we force it to fit with all the predictors, and some will fit to spurious patterns in the training data, worsening the predictions on new (unseen) data.
But a tree model will simply ignore those predictors and choose the relevant predictors instead. So no worries then?

Here, we show that Random Forest (a tree-based model that performs well in many settings) can still be harmed by irrelevant features, and offer two ways of dealing with it. We can do so either by removing the irrelevant features (using a procedure called **recursive feature elimination (RFE)**), or by **tuning the algorithm**, increasing the number of features available during each split (the `mtry` parameter in R) during training (model fitting). 

That tuning `mtry` for a Random Forest is important in the presence of many irrelevant features was already shown by Hastie et al. [@hastie_elements_2009] in 2009. They showed that if `mtry` is kept at its default  (square root of $p$), as more irrelevant variables are added, the probability of the **relevant** features being selected for splitting becomes too low, decreasing performance. So for datasets with a large proportion of irrelevant features, `mtry` tuning (increasing its value) is crucially important.

The classic book *Applied Predictive Modeling* by Kuhn and Johnson (2013) contains a similar experiment showing the negative effects of including many irrelevant features. However, instead of tuning RF, they suggest **removing the irrelevant features altogether**. 

Here I show, using simulated data with increasing amounts of noise variables, it appears that when the amount of **noise** (irrelevant features) increases relative to the **signal** (relevant features), at some point the RF tuning approach no longer is able to achieve optimal performance. Under such (possibly extreme) circumstances, feature selection using RFE results in a significantly higher predictive performance compared to RF tuning. 

As a corollary, it seems that **combining** feature elimination and tuning does not make much sense, as the default settings of Random Forest likely are close to optimal after removing the irrelevant features. 

<!--We nevertheless provide R code using `caret` for doing so. We show how this can be achieved with the `rfe()` framework of `caret`, by creating a set of custom helper functions for `rfe()`. -->

Finally, we echo others in stressing the importance of not refitting the same data after feature selection, to avoid an overly optimistic assessment of out-of-sample performance. 

# What is overfitting

Here we are dealing with a special kind of overfitting, overfitting to predictors. 

So what are we talking about when we state that algorithm XYZ does not overfit? An algorithm is said to **overfit** when it is sufficiently flexible to fit patterns in the training set that are absent in a new batch of data. With "a new batch of data" we mean data that was generated according to the same Data generating process (DGP) as the training data, but was not used in model building. Such data is often called "test data". Overfitting causes model prediction errors to be higher on test data compared to the prediction errors on the training data.

In the context of random forest, there is a additional claim related to overfitting, that has caused confusion. The claim by the inventor of the algorithm, Leo Breiman, himself, taken out of context, is that "random forest does not overfit". [This blog post by Piotr Płoński](https://mljar.com/blog/random-forest-overfitting/) is a good write up on that claim (as well as on the overfitting decribed above) . The claim is that adding more trees to the forest does not hurt the predictions, but only improves them. In practice, the default setting of `ranger()`, 500 trees, is sufficient to get stable predictions, and adding even more is not worth the extra computational cost.

# Why Random Forest is my favorite ML algorithm

The Random Forest algorithm ([@breiman01]) is my favorite ML algorithm for cross-sectional, tabular data. Thanks to [Marvin Wright](https://mnwright.github.io/) a fast and reliable implementation exists for R called `ranger`[@wright17]. Compared to other popular ML methods, such as Deep learning or Gradient Boosting algorithms such as **XGBoost**, RF seems to offer the highest value per unit of compute, and in absolute terms generates predictions that often better than logistic regression [@Coronne18] often are either on par with Neural networks or Gradient boosting, or only slightly worse [@ref]. 

The Random Forest algorithm can provide a quick benchmark for the predictive performance of a set of predictors, that is hard to beat with models that explicitly formulate a interpretable model of a dependent variable, for example a linear regression model with interactions and non-linear transformations of the predictors.

For a great talk on the Random Forest method, check out [Marvin Wright's UseR talk from 2019](https://www.youtube.com/watch?v=iVmsJJYjgNs). Interestingly, Marvin evaluates various claims that also feature in this blog: For example the claim from [@Breiman01] that "RF does not suffer from overfitting". Here the precise definition of overfitting matters, as we shall see. 

# Random forest tuning versus feature selection

The random Forest is often said to perform already very well out-of-the-box, with default settings. This is corroborated by results of Philippe Probst, who wrote an R package to tune Random Forest, called `tuneRanger` [Github repo](https://github.com/PhilippPro/tuneRanger).  Probst created a suite of 29 regression datasets (OpenML-Reg-19), where he compared tuned ranger with default ranger. Indeed, most datasets hardly improve upon tuning, but a few did, and three stood out (blue line above dashed line). We return to these datasets later. Interestingly, the datasets he used are made available by [OpenML.org](https://new.openml.org/). This is a website dedicated to reproducible, open ML, with a large collection of datasets, focused on benchmarking and performance comparison. 

![](rf_rfe_post/probst_tuning_ranger.png)

With the exception of the `caret` project by Max Kuhn et al, it seems that most practical guidance to improve RF performance is on *tuning the algorithm hyperparameters*, arguing that Random Forest as a tree-based method has built-in feature selection, alleviating the need to remove irrelevant features. This is demonstrated by the many guides on (RF/ML) algorithm tuning found online. For example, a currently popular book "Hands-On Machine Learning" by Géron contains a short paragraph on the importance of selecting / creating relevant features, but then goes on to discuss hyperparameter tuning at great length for the remainder of the book. 

In this blog post, I will compare both RF **hyperparameter tuning** and **feature selection** in the presence of many irrelevant features.

# Random forest hyperparameter tuning: which parameters to tune?

In this blog post, we use `mtry` as the only tuning parameter of Random Forest. In the three regression examples of Probst et al where tuning showed a large difference (see figure above), tuning only `mtry` was sufficient to get close to optimal performance.

With respect to the other parameters, I left the number of trees at its default (500), and chose "variance" as the splitrule (alternative is "extratrees" which is more recent but I have yet to see convincing results that demonstrate it is superior). 

Do we have to tune the number of trees?
https://stats.stackexchange.com/questions/50210/caret-and-randomforest-number-of-trees

I also played around a bit with the `min.node.size` parameter, for which often the sequence 5,10,20 is mentioned to vary over. Setting this larger should reduce computation, since it leads to shorter trees, but for the datasets here, the effect is on the order of say 10% reduction, which does not warrant tuning it. I left this at its default of 5 for regression.


# Recursive Feature elimination

For this, we use the recursive feature elimination (RFE) procedure, implemented for `ranger` in `caret`.
As this is procedure that drops predictors that do not correlate with the outcome, we have to be extremely careful that we end up with something that generalizes to unseen data.


    When the full model is created, a measure of variable importance is computed that ranks the predictors from most important to least. […] At each stage of the search, the least important predictors are iteratively eliminated prior to rebuilding the model.

— Pages 494-495, Applied Predictive Modeling, 2013.

PM: als we toch niet refitten, waarom dan eigenlijk niet van de andere kant komen, dus inderdaad ascending a la Genuer. Svetnik et al (2004) showed that, for random forest models, there was a decrease in performance when the rankings were re-computed at every step.

We therefore performed recursive feature elimination (RFE) with Random Forest for all models in order to obtain models with optimal predictive performance. The `rfe()` function of the caret package in R was used, with the `rangerFuncs` set of helper functions (Kuhn book)3. Permutation importance was used as the variable importance mode. The Root Mean Squared Error (RMSE) was the performance metric used to choose the optimal subset size of the variables. Ten different subset sizes were tested by the RFE. These ranged from the first to the tenth decile of the number of variables when the model contained more than 10 variables; for models containing less than or exactly 10 variables, all possible subset sizes were considered.

In the book **Applied predictive Modeling** the authors convincingly show that a special procedure is necessary, with two loops of cross validation. The outer loop sets aside one fold that is not used for feature selection (and optionally model tuning), whereas the inner loop selects features and tunes the model.

What happens if we do not follow this procedure, and refit the model using only the selected features from the selection procedure. To make sure there are no patterns in the data, we shuffle the outcome.

PM it is possibly not OK to refit / tune after / outside the RFE loop.

"If the model is refit using only the important predictors, model performance almost certainly improves"

<!-- While mtry is a tuning parameter for random forest models, the default value of mtry≈sqrt(p) tends to provide good overall performance. While tuning this parameter may have led to better performance, our experience is that the improvement tends to be marginal -->

# Finding suitable datasets: Probst suite of datasets for ML regression benchmarking

PM hier over welke datasets

For this blog post we need datasets to demonstrate the behavior of RF and the various packages we use to train and tune the models. In particular, I needed a dataset that is not too big, nor too small, and where Random Forest tuning has a substantial effect. Using simulated data is always an option, but with such data it is not always clear what the practical significance of our findings is. 

I therefore put in some effort to find a few real datasets as well where RF tuning has a substantial effect. 

This was not easy: Surprisingly, online tutorials for RF hyperparameter tuning often only show small improvements in performance. Here the benchmarking study of Probst on RF tuning came to the rescue, as he identified three datasets (out of 29) where RF tuning has a significant effect. For these datasets, R-squared increases of 6%, 7% and 25% percent points were observed. The 25% point increase was in a synthetic dataset called `puma32H`, going from 67% to 93% after tuning. 

*Interestingly, what these datasets have in common that they all contain a large proportion of irrelevant features.*

The p:n  ratio is also important for showcasing these effects. If there is a lot of data, the models can withstand a lot of noise.
Because there is also a lot of signal present. The probability than 10.000 random noise points show a correlation with the outcome is much smaller than for 100 datapoints.

# Random forest does automatic feature selection?

Some models are negatively affected by non-informative predictors. Simply using all the predictors available will negatively impact performance for some "learner" methods. For example, linear regression (OLS) automatically includes all supplied variables in the final model, and this can cause the trained model to perform worse on unseen data. This is a form of overfitting: ideally variables that do not contribute to the prediction should have their weights set to zero, effectively excluding them from the model. For linear regression, the elastic net / LASSO / ridge regression variants have this.

Other models are naturally resistant to non-informative predictors. For example, decision tree models implicitly perform feature selection by not including non-informative predictors. Random Forest, as a tree-based method, is to some extent resistant to the presence of irrelevant variables in the model [ref? this post?]. However, if more irrelevant variables are added to a Random Forest model, at some point its performance will be adversely affected [@genuer_variable_2010]. 


Another common claim about RF that Marvin takes on is that "Random Forest works well on high-dimensional data". High-dimensional data is related to the ratio between number of observations and number of variables. Data is called high-dimensional if there are many variables relative to the number of observations. This is common in genetics, when we have say complete genomes for only a handful of subjects. The suggestion is that RF can be used on small datasets with lots of features without having to do variable selection first. 

He quotes theoretical results from [Biau and Scornet 2016 (A random forest guided tour)]() who write: 

```
the rate of convergence only depends on the number of strong variables S, not on the dimension of the data p....[]
```
However, it turns out that a strong assumption is needed for this claim to hold: 

```
Of course, this is achieved by assuming that the procedure succeeds in selecting the informative variables for splitting. which is indeed a strong assumption.
```

In his presentation, Wright shows that RF performance is unaffected by adding 100 noise variables to the `iris` dataset, a famous example classification problem with three different species. The fact the Random Forest performs "intrinsic variable selection" at each split is used to explain this result. 

Let's try that ourselves as well.

# Adding irrelevant variables to the Iris dataset

First the benchmark, using `ranger()` on the `iris` dataset, with 10-fold Cross validation:


We get an cross-validated accuracy of 95-96% (so CV error of around 5%).

We add a 100 irrelevant predictors to the `iris` dataset (I wrote a function `add_spurious_predictors()` for this), and check whether accuracy suffers.


We can see that if we do not choose really low values of `mtry`, performance hardly suffers, with 94-96% Accuracy.
So indeed, we confirm the conclusion of Wright: For the `iris` classification problem, Random Forest does not suffer when we add a lot of noise variables, and using the default settings (`mtry = p/3`).

However, it turns that it is easy to generate a counter-example where RF severely overfits.

# OpenML-Reg19

<!--
In all three cases, setting `mtry` to its maximum value gave me close to optimal performance. 
So we find that for the puma32H dataset, EITHER tuning mtry and retaining all predictors, OR using RFE and not tuning mtry result in equal performance. Both aim to reduce the effect of irrelevant variables.

```
The strongest increase of the bias of ET1 is observed on Friedman1 and Pumadyn-32nm.
Actually, these two problems have a large proportion of irrelevant attributes (5 out of 10
on Friedman, 30 out of 32 on Pumadyn-32nm). The effect of removing them is analyzed
in Section 4.4.[@geurts06] 
```
-->

# An example where Random Forests overfit

This example is taken from the documentation of `caret`, by Max Kuhn, in particular the section on [RFE](https://topepo.github.io/caret/recursive-feature-elimination.html). In that section, no tuning is done, which means that the most RF important tuning parameter `mtry` is determined by $p/3$.

PM Here we repeat the analysis using `ranger`, and also include tuning of `mtry`, to rule out that the overfitting is caused by a poor choice of hyperparameters.

`mlbench.friedman1()` simulates the regression problem **Friedman 1** as described in Friedman (1991) and Breiman (1996) [@Friedman91] [@Breiman96]. Inputs are 10 independent variables uniformly distributed on the interval [0,1], only 5 out of these 10 are actually used. Outputs are created according to the formula:

$y = 10 sin(\pi x_1 x_2) + 20 (x_3 - 0.5)^2 + 10 x_4 + 5 x_5 + \epsilon$

where $\epsilon$ is distributed $Normal(0, 1)$.



First we establish the performance of the optimal model, including only relevant predictors X1-X5:


So the optimal model has R-squared of around 75-80%. Interestingly, even with only relevant variables, tuning `mtry` results in an average improvement of a few % points R-squared.

Now after including the 100+5 irrelevant variables:


The horror! R-squared has dropped to around 63%. Note that tuning the `mtry` is important, with the optimal value somewhere around $p/3 = 36$. 

Does this rule of thumb to choose `mtry` keep being optimal, even if we add more irrelevant variables?


Surprisingly, it appears so! So there might not be much benefit to tuning when doing RFE using Random Forest.
Note that with 500+5 noise variables, R-squared has dropped further to 48%.

Summary so far: adding irrelevant predictors has a big negative effect, tuning mtry has a small effect, and the default of p/3 seems to perform pretty well.

```{r}
caret::var_seq(510, len = 1)
```

# PM rewrite

In https://github.com/topepo/FES_Selection_Simulation/blob/master/files/effects_0500_000_3758.R finds that RF is robust to 200 extra noise, with 20 signal. He uses ranger, WITH tuneLength = 25. So that means that `mtry` is tuned, and this solves the problem.
He mentions that feature selection can still be used to get a smaller model.


# What if we do not know which variables are irrelevant? RFE

The `caret` package has the `rfe()` function for us, with a set of helper functions for the original `RandomForest` package. This fits a random forest using `mtry = p/3` so **without any tuning**. 


Still confused: what happens in the inner loop?

We have 2 fold CV as external loop. For each fold, we do: fitting model, ranking, then fitting the subsets. Everytime we check performance on the held out fold.

```{r}
rfFuncs$fit <- function (x, y, first, last, ...) 
{
    loadNamespace("randomForest")
    print(dim(x))
    randomForest::randomForest(x, y, importance = TRUE, ...)
}

ctrl <- rfeControl(functions = rfFuncs,
                   method = "cv",
                   number = 2,
                   returnResamp = "all",
                   verbose = FALSE,
                   saveDetails = TRUE)

#subsets <- c(1:10, 150, 200, 300)
subsets <- c(1:10, 30, 60, 90)

set.seed(10)
rfProfile <- rfe(y ~ ., 
                 data = df, 
                 metric = "Rsquared",
                 sizes = subsets, 
                 rfeControl = ctrl)

plot(rfProfile)
```

```{r}
rfProfile$fit$mtry
```


And BAM! It has identified four of the five relevant features, giving us a model that has 79% R-squared, just as our optimal benchmark that included X1 to X5. Default mtry setting was floor(4/3) = 1.

Finally, we switch to `ranger()`, use permutation importance instead of the faster impurity ("variance") and introduce tuning for `mtry`. Dataset is 5+105 irrelevant.

```{r}
source("rf_rfe_post/rangerTuneFuncs.R")

RFE_ctrl <- rfeControl(functions = rangerTuneFuncs,
                   method = "cv",
                   number = 2,
                   rerank = FALSE,
                   returnResamp = "all",
                   verbose = TRUE,
                   saveDetails = TRUE)

# Tuning the model after feature selection

train_ctrl <- trainControl(method = "cv",
                           selectionFunction = "tolerance",
                           returnResamp = "all",
                           verboseIter = TRUE,
                           savePredictions = TRUE)

#subsets <- c(1:10, 30, 60, 90)
subsets <- c(5)

set.seed(10)
rfProfile2 <- rfe(y ~ ., 
                 data = df, 
                 metric = "Rsquared",
                 sizes = subsets, 
                 rfeControl = RFE_ctrl,
                 trControl = train_ctrl)

plot(rfProfile2)
```

```{r}
rfProfile2$fit
```

We realize that combining RFE and tuning reduces the observations available.
In the outer loop, 10 observations are set apart, leaving 90 observations for the inner loop.

Also: 

<!------------------- MAAK TEST SET VOOR FRIEDMAN------------->
<!--------------- SHUFFLE Y -->


# Conclusions

It must be noted that this is a relatively clean classification problem where the classes are well separated in feature space. 

# References

[@Probst18] Hyperparameters and Tuning Strategies for Random Forest

[@Couronne18] Random forest versus logistic regression: a large-scale benchmark experiment

[@genuer_variable_2010] variable selection using random forests

The main contribution is twofold: to provide some insights about the behavior of the variable importance index based on random forests and to propose a strategy involving a ranking of explanatory variables using the random forests score of importance and a stepwise ascending variable introduction strategy
<!-- four classification, one regression, Ozone -->

[@Ambroise02] [Selection bias in gene extraction on the basis of microarray gene-expression data](https://www.pnas.org/content/99/10/6562.short)

[@svetnik03] Svetnik, V, A Liaw, C Tong, C Culberson, R Sheridan, and B Feuston. 2003. 
Random Forest: A Classification and Regression Tool for Compound Classification and QSAR Modeling

There is no evidence that performance actually improves with descriptor selection. Also, the default mtry has the best
performance. 
[...]
However, it is still important for users to investigate the sensitivity of Random Forest to changes
in mtry or descriptor selection, as illustrated in the COX-2 classification data, where mtry = p turned out to be the best
choice. For further discussion of descriptor selection with Random Forest, see ref 33.

The exceptional case is the COX-2 classification data. In this case, mtry = p (bagging) is the best performer, as can be seen in Figure 3, although descriptor reduction (RFE) only degrades performance.

[@svetnik04] Svetnik, V.; Liaw, A.; Tong, C. Variable selection in random forest with application to quantitative structure-activity relationship. In Proceedings of the 7th Course on Ensemble Methods for Learning Machines; Intrator, N., Masulli, F., Eds.; Springer-Verlag: Berlin, 2004; submitted

It is shown that the non-recursive version of the procedure outperforms the recursive version, and that the default Random Forest mtry function is usually adequate.

[@svetnik04] Application of Breiman’s Random Forest to Modeling Structure-Activity Relationships of Pharmaceutical Molecules

Without any parameter tuning, the performance of Random Forest with default settings on six publicly available data sets is already as good or better than that of three other prominent QSAR methods: Decision Tree, Partial Least Squares, and Support Vector Machine. In addition to reliable prediction accuracy, Random Forest provides variable importance measures which can be used in a variable reduction wrapper algorithm. (**RFE**)

*We have never yet observed a case where the performance actually improves as variables are reduced*

[@kuhn13] Applied Predictive Modelling, with Tidymodels companion at https://github.com/topepo/tidy-apm

[@Kuhn18] Feature Engineering and Selection: A Practical Approach for Predictive Models, online at 
https://bookdown.org/max/FES

Book online:
https://bookdown.org/max/FES/feature-selection-simulation.html

Simulation irrelevant features:
https://github.com/topepo/FES_Selection_Simulation

<!--# TuneRanger

Show that TuneRanger tunes the mtry value.

```{r}
library(tuneRanger)
library(mlr)

monk_data_1 = getOMLDataSet(505)$data
monk.task = makeRegrTask(data = monk_data_1, target = "fat")

# Estimate runtime
estimateTimeTuneRanger(monk.task)
# Approximated time for tuning: 1M 13S
set.seed(123)
# Tuning
res = tuneRanger(monk.task, num.trees = 500,
num.threads = 2, iters = 70, iters.warmup = 30)
res
```
-->