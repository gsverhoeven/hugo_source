---
title: "How to prevent overfitting with Random Forests"
author: "Gertjan Verhoeven"
date: '2022-01-03'
summary: This blog post demonstrates that the Random Forest algorithm (as implemented in R by `ranger()`) suffers from overfitting by reproducing an example from the book *Applied Predictive Modeling*. The solution is to use Recursive Feature Elimination (RFE) to remove predictors with spurious correlations. We provide up to date code that can be used for workflows that use either `caret` and/or `tidymodels` .
slug: random-forest-overfitting
draft: no
categories: 
  - Random Forest
  - Machine Learning
  - Overfitting
tags:
  - caret
  - tidymodels
  - ranger
baseurl: https://gsverhoeven.github.io
header:
  image: headers/wilhelm-gunkel-di8ognBauG0-unsplash.png
  preview: no
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(AppliedPredictiveModeling)
library(caret)
library(tidyverse)

```

The Random Forest algorithm can provide a quick benchmark for the predictive performance of a set of predictors, that is hard to beat with models that explicitly formulate a interpretable model of a dependent variable, for example a linear regression model with interactions and non-linear transformations of the predictors.

    When the full model is created, a measure of variable importance is computed that ranks the predictors from most important to least. […] At each stage of the search, the least important predictors are iteratively eliminated prior to rebuilding the model.

— Pages 494-495, Applied Predictive Modeling, 2013.

data leakage


Random Forest is to a large extent immune to the presence of irrelevant variables in the model. However, if more irrelevant variables are added to a Random Forest model, at some point its performance will be adversely affected [@hastie_elements_2009; @genuer_variable_2010]. We therefore performed recursive feature elimination (RFE) with Random Forest for all models in order to obtain models with optimal predictive performance. The rfe() function of the caret package in R was used, with the rangerFuncs set of helper functions (Kuhn book)3. Permutation importance was used as the variable importance mode. The Root Mean Squared Error (RMSE) was the performance metric used to choose the optimal subset size of the variables. Ten different subset sizes were tested by the RFE. These ranged from the first to the tenth decile of the number of variables when the model contained more than 10 variables; for models containing less than or exactly 10 variables, all possible subset sizes were considered.


However, simply using all the predictors 

scikit-learn RFE.

This blog post demonstrates that the Random Forest algorithm (as implemented in R by `ranger()`) suffers from overfitting by reproducing an example from the book *Applied Predictive Modeling*. This reduces model performance on the test set.

The solution is to use **Recursive Feature Elimination (RFE)** to remove predictors with spurious correlations. We provide up to date code for R using `caret`  and `tidyverse`.



Some models are negatively affected by non-informative predictors.
Some models are naturally resistant to non-informative predictors.
Tree based models implicitly perform feature selection by not including non-informative predictors.

19.1 QSAR data, adding 10,50,100,200,300,400,500 non-informative predictors

test set profiles were calculated.

Do we have to tune the number of trees?
https://stats.stackexchange.com/questions/50210/caret-and-randomforest-number-of-trees

This code reproduces Fig 8.18 from Max Kuhn's book Applied Predictive Modelling from 2013.

as well as Fig 19.1

Ref https://www.pnas.org/content/99/10/6562.short

Tidymodels companion at https://github.com/topepo/tidy-apm

https://bookdown.org/max/FES/feature-selection-simulation.html

https://github.com/topepo/FES_Selection_Simulation

# Tuned model without removing spurious features

```{r}
data(solubility)

test <- 0

if(test){
  # replace full training dataset with a smaller subset
  set.seed(123)
  ss <- sample(x = 1:nrow(solTrainXtrans), size = 100, replace = F)
  
  solTrainXtrans <- solTrainXtrans[ss,]
  solTrainY <- solTrainY[ss]
}
```

Description of the dataset:

```
Tetko et al. (2001) and Huuskonen (2000) investigated a set of compounds with corresponding experimental solubility values using complex sets of descriptors. They used linear regression and neural network models to estimate the relationship between chemical structure and solubility. For our analyses, we will use 1267 compounds and a set of more understandable descriptors that fall into one of three groups: 208 binary "fingerprints" that indicate the presence or absence of a particular chemical sub-structure, 16 count descriptors (such as the number of bonds or the number of Bromine atoms) and 4 continuous descriptors (such as molecular weight or surface area).
```

75% of the data was used for training the models, and 25% of the data was used for testing the final model performance.

```{r}
set.seed(123)

indx <- createFolds(solTrainY, returnTrain = TRUE)

# use 10-fold CV
ctrl <- trainControl(method = "cv", 
                     index = indx, 
                     verboseIter = FALSE)

rangerGrid <- data.frame(mtry = floor(seq(10, ncol(solTrainXtrans), length = 10)),
                       splitrule = "variance", 
                       min.node.size = 5)

```

```{r}
fullrun <- 0

if(fullrun){
  set.seed(100)
  
  rfTune2 <- train(x = solTrainXtrans, y = solTrainY,
                  method = "ranger",
                  tuneGrid = rangerGrid,
                  trControl = ctrl)
  
  saveRDS(rfTune2, file = "rf_rfe_post/rfTune2_ss.rds")
} else {
  rfTune2 <- readRDS(file = "rf_rfe_post/rfTune2_ss.rds")
}

plot(rfTune2)

```

```{r}
rfTune2
```
So there we have it: a tuned Random Forest model, using 228 predictors (properties of the molecules) to predict the solubility of a chemical compound.


```{r}
### Save the test set results in a data frame                 
testResults <- data.frame(obs = solTestY,
                          RFtune2 =  predict(rfTune2, solTestXtrans), n_spurious = 0)

```

Lets calculate the Test set RMSE.

```{r}
library(yardstick)

metrics(testResults, truth = obs, estimate = RFtune2)
```
From here, we can go either way: we can add uninformative predictors, and check if performance suffers, or we can try to  remove uninformative predictors to further improve the model. 
We start with adding noise predictors to check if performance suffers.
If so, then we have shown that the RF has overfitted the data.

PM definition of overfitting (should performance suffer? or difference between train and test performance)
Overfitting is anything that does not generalize to new samples.

# adding non-informative predictors

```{r}
source("rf_rfe_post/add_spurious_predictors.R")

spurious_vec <- c(10, 50, 100, 200, 300, 400, 500)

fullrun <- 0

for(i in 1:length(spurious_vec)){
  target <- paste0("rf_rfe_post/rfTune2_ss", spurious_vec[i], ".rds")
  set.seed(100)
  solTrainXtrans_spur <- add_spurious_predictors(solTrainXtrans, spurious_vec[i])

    if(fullrun){
    print(paste0("running with ", spurious_vec[i], " spurious vars added"))

    rfTune2 <- train(x = solTrainXtrans_spur, y = solTrainY,
                    method = "ranger",
                    tuneGrid = rangerGrid,
                    trControl = ctrl)
    
    saveRDS(rfTune2, file = target)
  } else {
    rfTune2 <- readRDS(file = target)
  }
  # add spurious predictors to test data as well
  solTestXtrans_spur <- add_spurious_predictors(solTestXtrans, spurious_vec[i])

  testResults <- rbind(testResults,
                       data.frame(obs = solTestY,
                       RFtune2 =  predict(rfTune2, solTestXtrans_spur), n_spurious = spurious_vec[i]))
}
```
This gives us a dataset with for each of the models build, with increasing amounts of spurious predictors added, predictions on the test set, i.e. the dataset that was not used in anyway during model building.

It has the following structure:

```{r}
testResults %>% 
  group_by(n_spurious) %>% 
  summarise(n())
```
Using the `yardstick` library, part of `tidymodels`, we can easily calculate performance metrics such as RMSE and R-squared for the prediction models.

```{r}
library(yardstick)

res <- testResults %>% 
  group_by(n_spurious) %>% 
  metrics(truth = obs, estimate = RFtune2)
```

```{r}
library(ggplot2)

ggplot(res %>% filter(.metric == "rmse"), aes(x = n_spurious, y = .estimate)) +
  geom_point() +
  geom_line() +
  expand_limits(y=c(0.6,1.2))
```
It is clear from the graph that the prediction error increases with more spurious predictors present.
However, if we look at the R-squared, and include zero on the y-axis, it is also apparent that the decrease in performance is relatively small, from 91% down to 86%. 

```{r}
ggplot(res %>% filter(.metric == "rsq"), aes(x = n_spurious, y = .estimate*100)) +
  geom_point() +
  geom_line() +
  expand_limits(y=0) +
  geom_label(aes(label = paste(round(.estimate*100,0), "%")), nudge_y = -5)
```

Now lets go the other way: we try to remove all the spurious predictors and leave only the informative predictors.

# Recursive feature elimination (RFE)

For this, we use the recursive feature elimination (RFE) procedure, implemented for `ranger` in `caret`.

PM it is possibly not OK to refit / tune after / outside the RFE loop.

"If the model is refit using only the important predictors, model performance almost certainly improves"


## RFE settings

We use repeated k-fold Cross Validation as resampling method. By repeating this a few times and averaging the results we get some more precision in our estimates. Both the Recursive Feature Elimination and the model tuning functions of `caret` require a Control object with settings. `PicksizeTolerance` is used both to select the best model with the least features, and when tuning the Random Forest model for the selected features. This way, the results are less sensitive to small fluctuations.

 Svetnik et al (2004) showed that, for random forest models, there was a decrease in performance when the rankings were re-computed at every step.
 
 So only the first run uses permutation importance to create the variable ordering.

```{r}
source("rf_rfe_post/rangerFuncs.R")

RFE_ctrl <- rfeControl(functions = rangerFuncs,
                   method = "repeatedcv",
                   rerank = F,
                   returnResamp = "all",
                   number = 10,
                   repeats = 1, 
                   verbose = FALSE, # creates a lot of output if set TRUE
                   saveDetails = T)

# Tuning the model after feature selection

train_ctrl <- trainControl(method = "repeatedcv",
                           number = 10, 
                           repeats = 1,
                           selectionFunction = "tolerance",
                           savePredictions = TRUE)

```

I wrote a wrapper function that hides some detail of the `caret` functions used, to focus on the actual settings and parameters. 

https://github.com/topepo/caret/issues/439

# Do the work

```{r}
source("rf_rfe_post/RunRFEandTuneRF.R")

fullrun <- 1
outdir = "rf_rfe_post/"
target <-  paste0(outdir, c("rfe_rf_tmp.rds"))
permutate_y <- FALSE

if(fullrun){
  set.seed(123)
  n_rows <- length(solTrainY)
  
  if(permutate_y) solTrainY <- solTrainY[sample(1:n_rows, size = n_rows, replace = F)]
  
  ss <- sample(1:n_rows, size = 100, replace = FALSE)
  #ss <- 1:n_rows
    
  res <- RunRFEandTuneRF(X = solTrainXtrans_spur[ss,], 
                         Y = solTrainY[ss], 
                         rfe_ctrl = RFE_ctrl, 
                         train_ctrl = train_ctrl,
                         seed = 123)
  saveRDS(res, target)

} else{
  res <- readRDS(target)
}

rfeObject <- res[[1]]
trainObject <- res[[2]]
```

A model with 74 predictors is found to be the smallest model with optimal performance.

```{r}
rfeObject$fit
```


```{r}
print(rfeObject)
```

```{r}
trainObject
```


```{r}
testResults <- rbind(testResults,
                     data.frame(obs = solTestY,
                     RFtune2 =  unlist(predict(trainObject, solTestXtrans_spur)), n_spurious = -1))
```

```{r}
res <- testResults %>% 
  group_by(n_spurious) %>% 
  metrics(truth = obs, estimate = RFtune2)

res
```
So we find that the RFE procedure identified a model with 74 predictors that has identical performance 90% R-squared as the model with all the 228 original predictors.

# Sanity check: shuffle the dependent variable column

As a sanity check, we shuffle the outcome variable column and repeat the same procedure.
This was also done is Ambroise et at.
We know now for a fact that there is no true predictive value in the set of predictors, since the 500 spurious predictors were already unrelated to the outcome, and after shuffling (permutating), the relationships (if any) between the original 228 predictors and the outcome should be lost as well.
All metrics should drop to zero, as there is no signal anymore to discover in the predictors.

It would be a worrying sign if we would be able to build a prediction model for the outcome.

# Do the work after permutation



```{r}
source("rf_rfe_post/RunRFEandTuneRF.R")

fullrun <- 0
outdir = "rf_rfe_post/"
target <-  paste0(outdir, c("rfe_rf_permute_tmp.rds"))
permutate_y <- TRUE

if(fullrun){
  set.seed(123)
  n_rows <- length(solTrainY)
  
  if(permutate_y) solTrainY <- solTrainY[sample(1:n_rows, size = n_rows, replace = F)]
  
  #ss <- sample(1:n_rows, size = 100, replace = FALSE)
  ss <- 1:n_rows
    
  res <- RunRFEandTuneRF(X = solTrainXtrans_spur[ss,], 
                         Y = solTrainY[ss], 
                         rfe_ctrl = RFE_ctrl, 
                         train_ctrl = train_ctrl,
                         seed = 123)
  saveRDS(res, target)

} else{
  res <- readRDS(target)
}

rfeObject <- res[[1]]
trainObject <- res[[2]]
```

The cross-validated model contains as top 5 predictors SPUR148, SPUR2, SPUR406, SPUR161, SPUR487, and has an cross validated R2 of 11%!!!
This is purely the result of our massive data dredging exercise, where we have screened 951 observations with 728 variables for correlation with an outcome variable. This demonstrates the value of keeping some of the data purely to safeguard us against such findings.

```{r}
rfeObject
```

```{r}
trainObject
```


PM large diff between RFE and RF R2


```{r}
testResults <- rbind(testResults,
                     data.frame(obs = solTestY,
                     RFtune2 =  predict(trainObject, solTestXtrans_spur), n_spurious = -10))
```

```{r}
res <- testResults %>% 
  group_by(n_spurious) %>% 
  metrics(truth = obs, estimate = RFtune2)

res
```

Indeed, for the test set, the R2 drops back to 0.0%. 

An alternative sanity check would be to shuffle the outcome of the train set and ... PM