---
title: "How to prevent overfitting with Random Forests"
author: "Gertjan Verhoeven"
date: '2022-01-03'
summary: This blog post demonstrates that the Random Forest algorithm (as implemented in R by `ranger()`) suffers from overfitting by reproducing an example from the book *Applied Predictive Modeling*. The solution is to use Recursive Feature Elimination (RFE) to remove predictors with spurious correlations. We provide up to date code that can be used for workflows that use either `caret` and/or `tidymodels` .
slug: random-forest-overfitting
draft: no
categories: 
  - Random Forest
  - Machine Learning
  - Overfitting
tags:
  - caret
  - tidymodels
  - ranger
baseurl: https://gsverhoeven.github.io
header:
  image: headers/wilhelm-gunkel-di8ognBauG0-unsplash.png
  preview: no
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(AppliedPredictiveModeling)
library(caret)
library(tidyverse)

```

This blog post demonstrates that the Random Forest algorithm (as implemented in R by `ranger()`) suffers from overfitting by reproducing an example from the book *Applied Predictive Modeling*. This reduces model performance on the test set. The solution is to use **Recursive Feature Elimination (RFE)** to remove predictors with spurious correlations. We provide up to date code for R using `caret`  and `tidyverse`. Importantly, we show that if both Recursive feature elimination **AND** model tuning is used **AND** the predictive accuracy of the final model is estimated on training data using cross-validation, a special nested cross-validation procedure is needed. We show how this can be achieved with the `rfe()` framework of `caret`, by creating a set of custom helper functions for `rfe()`.

# Random Forest algorithm

The random Forest algorithm is my favorite ML algorithm for cross-sectional, tabular data. Thanks to [Marvin Wright](https://mnwright.github.io/) a fast and reliable implementation exists for R. Compared to other popular ML methods, such as Deep learning or XGBoost, it seems the offer the highest value per unit of compute, and in absolute terms does it generate predictions that often are either on par with Neural networks or Gradient boosting, or only slightly worse. For a great talk on the method, check out [Marvin Wright's UseR talk from 2019](https://www.youtube.com/watch?v=iVmsJJYjgNs).

The Random Forest algorithm can provide a quick benchmark for the predictive performance of a set of predictors, that is hard to beat with models that explicitly formulate a interpretable model of a dependent variable, for example a linear regression model with interactions and non-linear transformations of the predictors.

# Introduction: Sensitivity to irrelevant predictors

Some models are negatively affected by non-informative predictors. Simply using all the predictors available will negatively impact performance for some "learner" methods. For example, linear regression (OLS) automatically includes all supplied variables in the final model, and this can cause the trained model to perform worse on unseen data. This is a form of overfitting: ideally variables that do not contribute to the prediction should have their weights set to zero, effectively excluding them from the model. For OLS, the elastic net / LASSO / ridge regression variants have this.

Other models are naturally resistant to non-informative predictors. For example, decision tree models implicitly perform feature selection by not including non-informative predictors.

For random forest, it is often said or assumed to be 
Random Forest is to a large extent immune to the presence of irrelevant variables in the model. However, if more irrelevant variables are added to a Random Forest model, at some point its performance will be adversely affected [@hastie_elements_2009; @genuer_variable_2010]. In the book *Applied Predictive Modeling*, by Max Kuhn and Kjell Johnson, the authors demonstrate this by adding more and more irrelevant predictors to an existing dataset related to molecular solubility, the `solubility` dataset.

# The dataset

Loading the `solubility` dataset gives us not one but six datasets! We only the transformed features (predictors, X-values), so we remove the datasets with the untransformed features, leaving us with four datasets:

* 228 predictors train dataset, 951 observations
* 228 predictors test dataset, 316 observations
* Vector of Y-values (the solubility), train set
* Vector of Y-values (the solubility), test set

```{r}
data(solubility)
rm(solTrainX, solTestX)
```

Description of the dataset:

```
Tetko et al. (2001) and Huuskonen (2000) investigated a set of compounds with corresponding experimental solubility values using complex sets of descriptors. They used linear regression and neural network models to estimate the relationship between chemical structure and solubility. For our analyses, we will use 1267 compounds and a set of more understandable descriptors that fall into one of three groups: 208 binary "fingerprints" that indicate the presence or absence of a particular chemical sub-structure, 16 count descriptors (such as the number of bonds or the number of Bromine atoms) and 4 continuous descriptors (such as molecular weight or surface area).
```

75% of the data was used for training the models, and 25% of the data was used for testing the final model performance. 





19.1 QSAR data, adding 10,50,100,200,300,400,500 non-informative predictors

test set profiles were calculated.

Do we have to tune the number of trees?
https://stats.stackexchange.com/questions/50210/caret-and-randomforest-number-of-trees

This code reproduces Fig 8.18 from Max Kuhn's book Applied Predictive Modelling from 2013.

as well as Fig 19.1

Ref https://www.pnas.org/content/99/10/6562.short

Tidymodels companion at https://github.com/topepo/tidy-apm

https://bookdown.org/max/FES/feature-selection-simulation.html

https://github.com/topepo/FES_Selection_Simulation

# Tuned ranger model without removing spurious features

In this blog post, we use `mtry` as the only tuning parameter of Random Forest, and choose "variance" as the splitrule (alternative is "extratrees" which is more recent but to my knowledge not necessarily better, on this dataset it seems worse, and takes more compute)

```{r}
set.seed(123)

indx <- createFolds(solTrainY, returnTrain = TRUE)

# use 10-fold CV
ctrl <- trainControl(method = "cv", 
                     index = indx, 
                     verboseIter = TRUE)

rangerGrid <- data.frame(mtry = floor(seq(10, ncol(solTrainXtrans), length = 10)),
                       splitrule = "variance", 
                       min.node.size = 10)

```

```{r}
fullrun <- 0

if(fullrun){
  set.seed(100)
  begin_time <- Sys.time()
  rfTune2 <- train(x = solTrainXtrans, y = solTrainY,
                  method = "ranger",
                  tuneGrid = rangerGrid,
                  trControl = ctrl)
  end_time <- Sys.time()
  print(end_time - begin_time)
  saveRDS(rfTune2, file = "rf_rfe_post/rfTune2_ss.rds")
} else {
  rfTune2 <- readRDS(file = "rf_rfe_post/rfTune2_ss.rds")
}

plot(rfTune2)

```
Random Forest 

951 samples
228 predictors

No pre-processing
Resampling: Cross-Validated (10 fold) 
Summary of sample sizes: 855, 856, 855, 857, 857, 855, ... 
Resampling results across tuning parameters:

  mtry  RMSE       Rsquared   MAE      
   10   0.8588962  0.8480163  0.6580583
   34   0.7524335  0.8750395  0.5699110
   58   0.7311974  0.8799670  0.5516170
   82   0.7179389  0.8831068  0.5367177
  106   0.7123788  0.8843594  0.5310332
  131   0.7105462  0.8843946  0.5295748
  155   0.7066657  0.8855717  0.5249173
  179   0.7051144  0.8855549  0.5225659
  203   0.7025755  0.8864378  0.5198557
  228   0.6993537  0.8873945  0.5169941

Tuning parameter 'splitrule' was held constant at a value of extratrees
Tuning parameter 'min.node.size'
 was held constant at a value of 20
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were mtry = 228, splitrule = extratrees and min.node.size = 20.

Extratrees took 10 min.
Fitting mtry = 228, splitrule = extratrees, min.node.size = 20 on full training set
Time difference of 9.450788 mins

Fitting mtry = 155, splitrule = variance, min.node.size = 20 on full training set
Time difference of 5.369846 mins

Fitting mtry = 131, splitrule = variance, min.node.size = 5 on full training set
Time difference of 7.595714 mins

```{r}
rfTune2
```
So there we have it: a tuned Random Forest model, using 228 predictors (properties of the molecules) to predict the solubility of a chemical compound. We find that `mtry` should be at least 82.


```{r}
### Save the test set results in a data frame                 
testResults <- data.frame(obs = solTestY,
                          RFtune2 =  predict(rfTune2, solTestXtrans), n_spurious = 0)

```

Lets calculate the Test set RMSE.

```{r}
library(yardstick)

metrics(testResults, truth = obs, estimate = RFtune2)
```
So far there is no indication that Random Forest has overfitted, since the CV error on the train set is virtually identical to the Test set error (Although we do not know the uncertainty in this metric yet, we could bootstrap it though)

From here, we can go either way: 

* we can add uninformative predictors, and check if performance suffers, or 
* we can try to  remove uninformative predictors (using RFE) to further improve the model. 

We start with adding noise predictors to check if performance suffers.
If so, then we have shown that the RF CAN overfit the data, because it has fitted a pattern that does not generalize to new data.

PM definition of overfitting (should performance suffer? or difference between train and test performance)
Overfitting is anything that does not generalize to new samples.

# adding non-informative predictors

```{r}
ctrl <- trainControl(method = "cv", 
                     index = indx, 
                     verboseIter = FALSE)


```


PM increase

```{r}
source("rf_rfe_post/add_spurious_predictors.R")

spurious_vec <- c(10, 50, 100, 200, 300, 400, 500)

fullrun <- 0

for(i in 1:length(spurious_vec)){
  target <- paste0("rf_rfe_post/rfTune2_spur_mtry_", spurious_vec[i], ".rds")
  set.seed(100)
  solTrainXtrans_spur <- add_spurious_predictors(solTrainXtrans, spurious_vec[i])
  # create tuning grid
  TuneGrid <- data.frame(mtry = floor(seq(10, ncol(solTrainXtrans), length = 10)),
                   splitrule = "variance", 
                   min.node.size = 5)
    if(fullrun){
      start_time <- Sys.time()
      print(paste0("running with ", spurious_vec[i], " spurious vars added"))
  
      rfTune2 <- train(x = solTrainXtrans_spur, y = solTrainY,
                      method = "ranger",
                      tuneGrid = TuneGrid,
                      trControl = ctrl)
      
      saveRDS(rfTune2, file = target)
      end_time <- Sys.time()
      print(end_time - start_time)
  } else {
    rfTune2 <- readRDS(file = target)
  }
  # add spurious predictors to test data as well
  solTestXtrans_spur <- add_spurious_predictors(solTestXtrans, spurious_vec[i])

  testResults <- rbind(testResults,
                       data.frame(obs = solTestY,
                       RFtune2 =  predict(rfTune2, solTestXtrans_spur), n_spurious = spurious_vec[i]))
}
```


This gives us a dataset with for each of the models build, with increasing amounts of spurious predictors added, predictions on the test set with 316 observations, i.e. a dataset that was not used in any way during model building.

The `testResults` dataset has the following structure:

```{r}
testResults %>% 
  group_by(n_spurious) %>% 
  summarise(n())
```
Using the `yardstick` library, part of `tidymodels`, we can easily calculate performance metrics such as RMSE and R-squared for the prediction models.

```{r}
library(yardstick)

res <- testResults %>% 
  group_by(n_spurious) %>% 
  metrics(truth = obs, estimate = RFtune2)
```

```{r}
library(ggplot2)

ggplot(res %>% filter(.metric == "rmse"), aes(x = n_spurious, y = .estimate)) +
  geom_point() +
  geom_line() +
  expand_limits(y=c(0.6,1.2)) +
  ggtitle("RMSE increases as more irrelevant features are added")
```
It is clear from the graph that the prediction error increases with more spurious predictors present.
However, if we look at the R-squared, and include zero on the y-axis, it is also apparent that the decrease in performance is relatively small, from 91% down to 86%. 

```{r}
ggplot(res %>% filter(.metric == "rsq"), aes(x = n_spurious, y = .estimate*100)) +
  geom_point() +
  geom_line() +
  expand_limits(y=0) +
  geom_label(aes(label = paste(round(.estimate*100,0), "%")), nudge_y = -5) +
  ggtitle("R-squared decreases as more irrelevant features are added")
```
# mtry increases as more irrelevant features are added

```{r}

```


Now lets go the other way: we try to remove all the spurious predictors and leave only the informative predictors.

# Recursive feature elimination (RFE)

For this, we use the recursive feature elimination (RFE) procedure, implemented for `ranger` in `caret`.
As this is procedure that drops predictors that do not correlate with the outcome, we have to be extremely careful that we end up with something that generalizes to unseen data.


    When the full model is created, a measure of variable importance is computed that ranks the predictors from most important to least. […] At each stage of the search, the least important predictors are iteratively eliminated prior to rebuilding the model.

— Pages 494-495, Applied Predictive Modeling, 2013.


We therefore performed recursive feature elimination (RFE) with Random Forest for all models in order to obtain models with optimal predictive performance. The `rfe()` function of the caret package in R was used, with the `rangerFuncs` set of helper functions (Kuhn book)3. Permutation importance was used as the variable importance mode. The Root Mean Squared Error (RMSE) was the performance metric used to choose the optimal subset size of the variables. Ten different subset sizes were tested by the RFE. These ranged from the first to the tenth decile of the number of variables when the model contained more than 10 variables; for models containing less than or exactly 10 variables, all possible subset sizes were considered.

In the book **Applied predictive Modeling** the authors convincingly show that a special procedure is necessary, with two loops of cross validation. The outer loop sets aside one fold that is not used for feature selection (and optionally model tuning), whereas the inner loop selects features and tunes the model.

What happens if we do not follow this procedure, and refit the model using only the selected features from the selection procedure. To make sure there are no patterns in the data, we shuffle the outcome.

PM it is possibly not OK to refit / tune after / outside the RFE loop.

"If the model is refit using only the important predictors, model performance almost certainly improves"


# Wrong procedure: tuning after feature selection

We use repeated k-fold Cross Validation as resampling method. By repeating this a few times and averaging the results we get some more precision in our estimates. Both the Recursive Feature Elimination and the model tuning functions of `caret` require a Control object with settings. `PicksizeTolerance` is used both to select the best model with the least features, and when tuning the Random Forest model for the selected features. This way, the results are less sensitive to small fluctuations.

 Svetnik et al (2004) showed that, for random forest models, there was a decrease in performance when the rankings were re-computed at every step.
 
 So only the first run uses permutation importance to create the variable ordering.

```{r}
source("rf_rfe_post/rangerFuncs.R")

RFE_ctrl <- rfeControl(functions = rangerFuncs,
                   method = "repeatedcv",
                   rerank = FALSE,
                   returnResamp = "all",
                   number = 10,
                   repeats = 1, 
                   verbose = FALSE, # creates a lot of output if set TRUE
                   saveDetails = T)

# Tuning the model after feature selection

train_ctrl <- trainControl(method = "repeatedcv",
                           number = 10, 
                           repeats = 1,
                           selectionFunction = "tolerance",
                           savePredictions = TRUE)

```

I wrote a wrapper function that hides some detail of the `caret` functions used, to focus on the actual settings and parameters. 

https://github.com/topepo/caret/issues/439

## Do the work

```{r}
source("rf_rfe_post/RunRFEandTuneRF.R")

fullrun <- 0
outdir = "rf_rfe_post/"
target <-  paste0(outdir, c("rfe_rf.rds"))


if(fullrun){
  begin_time <- Sys.time()
  set.seed(123)
  n_rows <- length(solTrainY)
  
  #ss <- sample(1:n_rows, size = 100, replace = FALSE)
  ss <- 1:n_rows
    
  res <- RunRFEandTuneRF(X = solTrainXtrans_spur[ss,], 
                         Y = solTrainY[ss], 
                         rfe_ctrl = RFE_ctrl, 
                         train_ctrl = train_ctrl,
                         seed = 123)
  end_time <- Sys.time()
  print(end_time - begin_time)
  saveRDS(res, target)

} else{
  res <- readRDS(target)
}

rfeObject <- res[[1]]
trainObject <- res[[2]]
```

A model with 74 predictors is found to be the smallest model with optimal performance.

```{r}
rfeObject$fit
```


```{r}
plot(rfeObject)
```

```{r}
plot(trainObject)
```


```{r}
testResults <- rbind(testResults,
                     data.frame(obs = solTestY,
                     RFtune2 =  unlist(predict(trainObject, solTestXtrans_spur)), n_spurious = -1))
```

```{r}
res <- testResults %>% 
  group_by(n_spurious) %>% 
  metrics(truth = obs, estimate = RFtune2)

res
```
So we find that the RFE procedure identified a model with 58/74 predictors that has almost identical performance (89% vs 90%) R-squared as the model with all the 228 original predictors.

# Sanity check: shuffle the dependent variable column

As a sanity check, we shuffle the outcome variable column and repeat the same procedure.
This was also done is Ambroise et at.
We know now for a fact that there is no true predictive value in the set of predictors, since the 500 spurious predictors were already unrelated to the outcome, and after shuffling (permutating), the relationships (if any) between the original 228 predictors and the outcome should be lost as well.
All metrics should drop to zero, as there is no signal anymore to discover in the predictors.

It would be a worrying sign if we would be able to build a prediction model for the outcome.

## Do the work after permutation

```{r}
source("rf_rfe_post/RunRFEandTuneRF.R")

fullrun <- 0
outdir = "rf_rfe_post/"
target <-  paste0(outdir, c("rfe_rf_permute.rds"))
permutate_y <- TRUE

if(fullrun){
  set.seed(1234)
  begin_time <- Sys.time()
  n_rows <- length(solTrainY)
  
  if(permutate_y) solTrainY <- solTrainY[sample(1:n_rows, size = n_rows, replace = F)]
  
  ss <- 1:n_rows
    
  res <- RunRFEandTuneRF(X = solTrainXtrans_spur[ss,], 
                         Y = solTrainY[ss], 
                         rfe_ctrl = RFE_ctrl, 
                         train_ctrl = train_ctrl,
                         seed = 123)
  end_time <- Sys.time()
  print(end_time - begin_time)
  saveRDS(res, target)

} else{
  res <- readRDS(target)
}

rfeObject <- res[[1]]
trainObject <- res[[2]]
```

The cross-validated model contains as top 5 predictors SPUR148, SPUR2, SPUR406, SPUR161, SPUR487, and has an cross validated R2 of 3 / 6 / 11%!!! It depends on pickSizeBest vs pickSizeTolerance, because when only a few predictors are selected, prediction suffers.
This is purely the result of our massive data dredging exercise, where we have screened 951 observations with 728 variables for correlation with an outcome variable. This demonstrates the value of keeping some of the data purely to safeguard us against such findings.

```{r}
rfeObject
```

```{r}
trainObject
```

```{r}
testResults <- rbind(testResults,
                     data.frame(obs = solTestY,
                     RFtune2 =  predict(trainObject, solTestXtrans_spur), n_spurious = -10))
```

```{r}
res <- testResults %>% 
  group_by(n_spurious) %>% 
  metrics(truth = obs, estimate = RFtune2)

res
```

Indeed, for the test set, the R2 drops back to 0.0%. 



# DO RFE with Tuning included

Fig x shows the RFE algorithm as implemented in caret using the rfe() function.
It fits a fully tuned RF model 10 times, each time using 9 folds.
For this tuned RF model, permutation variable importance is calculated. This determines the order in which the features are eliminated. Then in 10 steps, the features are reduced, everytime retuning the ranger model.
This procedure results in an optimal number of features to select, and an optimal tuning parameter. This model is used to predict on the one fold that was not used at all, obtaining an unbiased estimate.
Then after doing this 10 times, estimates are pooled, and again 

For each step in the outer CV loop, 

```{r}
source("rf_rfe_post/RunRFEandTuneRF.R")
source("rf_rfe_post/rangerTuneFuncs.R")

RFE_ctrl <- rfeControl(functions = rangerTuneFuncs, #,
                   method = "repeatedcv",
                   rerank = FALSE,
                   returnResamp = "all",
                   number = 10,
                   repeats = 1, 
                   verbose = TRUE, # creates a lot of output if set TRUE
                   saveDetails = T)

train_ctrl <- trainControl(method = "repeatedcv",
                           number = 10, 
                           repeats = 1,
                           verboseIter = FALSE,
                           selectionFunction = "tolerance",
                           savePredictions = TRUE)

```

```{r}
fullrun <- 0
outdir = "rf_rfe_post/"
target <-  paste0(outdir, c("rfe_tune.rds"))
permutate_y <- FALSE

if(fullrun){
  set.seed(123)
  n_rows <- length(solTrainY)
  
  if(permutate_y) solTrainY <- solTrainY[sample(1:n_rows, size = n_rows, replace = F)]
  
  #ss <- sample(1:n_rows, size = 100, replace = FALSE)
  ss <- 1:n_rows
    
  res <- RunRFEwithTuning(X = solTrainXtrans_spur[ss,], 
                         Y = solTrainY[ss], 
                         rfe_ctrl = RFE_ctrl, 
                         train_ctrl = train_ctrl,
                         seed = 123)
  saveRDS(res, target)

} else{
  res <- readRDS(target)
}

rfeObject <- res[[1]]
```

```{r}
plot(rfeObject)
```

This took about 12 hours on two cores.
Remember that the mtry sequence was basically 1, max and something in between.


```{r}
testResults <- rbind(testResults,
                     data.frame(obs = solTestY,
                     RFtune2 =  predict(rfeObject, solTestXtrans_spur), n_spurious = -100))
```

```{r}
res <- testResults %>% 
  group_by(n_spurious) %>% 
  metrics(truth = obs, estimate = RFtune2)

res
```
