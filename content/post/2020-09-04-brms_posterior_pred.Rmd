---
title: The posterior distribution of an average treatment effect in Stan using brms::posterior_predict()
author: Gertjan Verhoeven & Misja Mikkers
date: '2020-09-04'
summary: Here we show how to calculate the posterior distribution of the adjusted average treatment effect for the covariates of a particular population using Stan & brms. 
slug: posterior-distribution-average-treatment-effect
draft: FALSE
categories:
  - R
tags:
  - brms, ate, causal effect, bayesian statistics
baseurl: "https://gsverhoeven.github.io"
header:
  image: "headers/treatment.png"
  preview: FALSE
---

# Introduction

Suppose we have data from a Randomized Controlled Trial (RCT). Patients get treated, or not, depending only on a coin flip.
The outcome is a count variable, such as the number of times the patient gets admitted to the hospital.
The treatment is expected to reduce the number of hospital admissions for patients.
As is often the case with patients, not all patients are identical :)
Suppose that older patients have more admissions. So age is a covariate.

# Average treatment effect (ATE)

Now, after we fitted a model to the data, we want to actually **use** our model to answer "What-if" questions (counterfactuals).
Here we answer the following question: 

* What would the average reduction in Admissions be if we had treated **ALL** the patients in the sample, compared to a situation where **NO** patient in the sample would have received treatment?

Well, that is easy, we just take the fitted model, change treatment from zero to one, and observe the ("marginal") effect on the outcome, right?

Yes, but the uncertainty is harder. We have uncertainty in the estimated coefficients of the intercept and covariate, as well us in the treatment variable. And these uncertainties can be correlated (for example between the coefficients of intercept and covariate).

Here we show how to use `posterior_predict()` to simulate outcomes of the model using the sampled parameters.
If we do this for two counterfactuals, all patients treated, and all patients untreated, and subtract these, we can easily calculate the posterior distribution of the average treatment effect!

# Packages used

This tutorial uses `brms`, allowing user-friendly bayesian modelling with Stan.

```{r, message = TRUE, warning = TRUE}
library(tidyverse)
library(rstan)
library(brms) 
```

# Data simulation

We generate fake data that matches our problem setup.

Admissions are determined by patient Age, whether the patient is treatment, and some random noise to capture unobserved effects that influence admissions.

```{r}
set.seed(123) 

id <- 1:200   
n_obs <- length(id)
b_tr <- 0.7
b_age <- 0.1

df_sim <- as.data.frame(id) %>% 
mutate(Age = rgamma(n_obs, shape = 5, scale = 2)) %>% # positive cont predictor
mutate(Error = rnorm(n_obs, mean = 0, sd = 0.5)) %>% # add noise
mutate(Treatment = ifelse(runif(n_obs) < 0.5, 0, 1)) %>% # Flip a coin for treatment
mutate(Lambda = exp(b_age * Age - b_tr * Treatment + Error)) %>% # generate lambda for the poisson dist
mutate(Admissions = rpois(n_obs, lambda = Lambda))

```


# Summarize data

Ok, so what does our dataset look like?

```{r}
summary(df_sim)
```

The Treatment variable should reduce admissions.
Lets visualize the distribution of Admission values for both treated and untreated patients:

```{r, warning=FALSE}
ggplot(data = df_sim, aes(x = Admissions)) +
  geom_histogram(stat="count") +
  facet_wrap(~ Treatment) + 
  theme_bw()
```

The effect of the treatment on reducing admissions is clearly visible. Now lets fit our Bayesian Poisson model to it.

# Fit model

We use `brms` default priors for convenience here. For a real application we would of course put effort into into crafting priors that reflect our current knowledge of the problem at hand.

```{r}
model1 <- brm(
  formula = as.integer(Admissions) ~  Age + Treatment,
   data = df_sim,
  family = poisson(),
  warmup = 2000, iter = 5000, 
  cores = 2, 
  chains = 4,
  seed = 123 
)
```
# Check model fit

```{r}
summary(model1)
```

We see that the posterior dists for $\beta_{Age}$ and $\beta_{Treatment}$ cover the true values, so were good.
To get a fuller glimpse into the (correlated) uncertainty of the model parameters we make a pairs plot:

```{r}
pairs(model1)
```

As expected, the coefficients $\beta_{Intercept}$ (added by `brms`) and $\beta_{Age}$ are highly correlated.

# Predict()

First we use `predict()` to calculate a treatment effect for each patient. This gives us a standard error for each patient, but we lose a lot of information we need to properly calculate the posterior distribution for the ATE.
But we can compare later on.

```{r}
df_sim_t0 <- df_sim %>% mutate(Treatment = 0)

df_sim_t1 <- df_sim %>% mutate(Treatment = 1)

df_sim_preds <- df_sim %>% mutate(Voorspellingt0 = 
                                    predict(model1, newdata = df_sim_t0))

df_sim_preds <- df_sim_preds %>% mutate(Voorspellingt1 = 
                                          predict(model1, newdata = df_sim_t1))

# ATE

tr_effs <- df_sim_preds[,8][,1] - df_sim_preds[,7][,1] 

mean(tr_effs)
```

Ok, so on average, our treatment reduces the number of Admissions by -1.9. 

But what is the range of plausible values consistent with the data & model?

We move on to `posterior_predict()`

# Posterior_predict()

`posterior_predict()` has two tricks:

The first trick is to take advantage of the fact that we have the full set of parameter draws from the posterior.
Conceptually, we imagine that each separate draw of the posterior represents a particular version of our model. 
In the example above, we have 12.000 samples from the posterior. Thus, we have 12.000 versions of our model, where unlikely parameter combinations occur less often compared to likely parameter combinations. The full uncertainty of our model parameters is contained in this "collection of models" . 

The second trick is that we simulate (generate) predictions from each of these 12.000 models.
Under the hood, this means computing for each model (we have 12.000), for each observation (we have 200) the predicted lambda value given the covariates, and drawing a single value from a Poisson distribution with that $\Lambda$ value (e.g. running `rpois(n = 1, lambda)` ).

This gives us a 12.000 x 200 matrix, that we can compute with.
For our application, the computation is simple: use `posterior_predict()` on our dataset with Treatment set to zero, do the same for our dataset with Treatment set to one, and subtract the two matrices.

```{r}
pp_t0 <- posterior_predict(model1, newdata = df_sim_t0)

pp_t1 <- posterior_predict(model1, newdata = df_sim_t1)

diff <- pp_t1 - pp_t0
```

Averaging over all datapoints and samples should give us the estimated treatment effect.

Compare to the fit parameters (intercept and treatment).
We calculate upper and lower bounds using the CI values from `brm`

```{r}
mean(diff)

est_t <- -0.89
est_intercept <- -0.02
est_pred_eff <- 0.1
avg_pred <- mean(df_sim$Predictor)

# brm fit parameters (intercept plus treatment)
mean_val <- exp(est_intercept + (est_pred_eff * df_sim$Predictor) +  est_t) - exp(est_intercept + (est_pred_eff * df_sim$Predictor))



#mean_val <- mean(mean_val)
plot(mean_val, mean_val_alt)
abline(0, 1)

mean(mean_val)
# use the Q5 Q95 vals for the model uncertainty (we ignore the uncertainty in the intercept, tricky to add)
#lower_val <- exp(-0.02+0.1*9.47 - 1.12) - exp(-0.02+0.1*9.47)
#upper_val <- exp(-0.02 + 0.1*9.47 - 0.68) - exp(-0.02+0.1*9.47)
```

Now average over all datapoints, but keep the samples dimension.
This is the estimated effect, FOR A PARTICULAR SAMPLE.

```{r}
mean_per_sample <- apply(diff, 1, mean)
```

The samples dimension now gives us the variability (uncertainty) of the estimate.

We compare with the Poisson model fit parameter for Treatment.

```{r}
ggplot(data.frame(mean_per_sample), aes(x = mean_per_sample)) +
  geom_histogram() + 
  geom_vline(xintercept = mean(mean_val))# +
#  geom_vline(xintercept = lower_val) +
#  geom_vline(xintercept = upper_val)
```
The variability in the samples is slightly higher than expected. This is likely due to us ignoring the uncertainty in the Intercept and Predictor.

# To conclude


