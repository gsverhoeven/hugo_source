---
title: Building TensorFlow 2.2 on an old PC
author: Gertjan Verhoeven
date: '2020-05-15'
summary: With the commoditization of deep learning in the form of Keras, I felt it was about time that I jumped on the Deep Learning bandwagon.
slug: deep-learning-tensorflow-keras
draft: FALSE
categories:
  - Machine learning
  - Deep learning
tags:
  - tensorflow
  - keras
  - neural networks
  - deep learning
baseurl: "https://gsverhoeven.github.io"
header:
  image: "headers/monolith.png"
  preview: FALSE
---

<link href="/rmarkdown-libs/anchor-sections/anchor-sections.css" rel="stylesheet" />
<script src="/rmarkdown-libs/anchor-sections/anchor-sections.js"></script>
<script src="/rmarkdown-libs/htmlwidgets/htmlwidgets.js"></script>
<script src="/rmarkdown-libs/viz/viz.js"></script>
<link href="/rmarkdown-libs/DiagrammeR-styles/styles.css" rel="stylesheet" />
<script src="/rmarkdown-libs/grViz-binding/grViz.js"></script>


<div id="introduction" class="section level1">
<h1>Introduction</h1>
<p>I don't change computers often. The fun for me is to make the most out of sparse resources. Linux fits nicely into this philosophy, because it can be adapted to run on really tiny computers (e.g. <a href="http://www.picotux.com/" class="uri">http://www.picotux.com/</a>), as well as huge supercomputers (<a href="https://itsfoss.com/linux-runs-top-supercomputers/" class="uri">https://itsfoss.com/linux-runs-top-supercomputers/</a>). I do like to keep up with new tech developments. And with the commoditization of deep learning in the form of Keras, I felt it was about time that I finally jumped on the Deep Learning bandwagon.</p>
<p>And the nice thing about lagging behind: The choice for deep learning is now extremely simple. I need <a href="https://keras.io/">Keras</a> with <a href="https://www.tensorflow.org/">TensorFlow</a> as a computational backend. Which nowadays means installing TensorFlow since the Keras API has been incorporated into the TensorFlow project.</p>
</div>
<div id="tensorflow-and-avx" class="section level1">
<h1>TensorFlow and AVX</h1>
<p>Then I ran into a problem: TensorFlow is all about FAST computation. And therefore it tries to exploit all hardware features that speed up computation. One obvious way to do so is utilizing specialized hardware such as GPU's and TPU's to do the number crunching. But even for CPU's, TensorFlow likes to make use of all the computational features that modern CPU's offer. One of these is the &quot;Advanced Vector Instruction Set&quot; , aka <a href="https://en.wikipedia.org/wiki/Advanced_Vector_Extensions">AVX</a>. As most CPU's from 2011 or later support AVX, the TensorFlow folks decided to only make binaries available that require a CPU with AVX. Bummer for me: as my CPU is from 2010, I needed to compile TensorFlow myself.</p>
<p>But come to think of it: What better rite of passage into the Deep Learning AI age is to compile TensorFlow from source on your own machine??? (Opening music of Space Odyssey 2001 in the background)</p>
</div>
<div id="building-tensorflow-on-a-really-old-computer" class="section level1">
<h1>Building TensorFlow on a really old computer</h1>
<p>I followed the <a href="https://www.tensorflow.org/install/source">tutorial from TensorFlow</a> to build from source on a Linux system (Ubuntu 18.04 LTS). Therefore, these notes are most useful to other Linux users, and my future self of course.</p>
<p>Roughly this consisted of:</p>
<ul>
<li>Creating a virtual environment for Python 3.6.9</li>
<li>Checking my GCC version (7.5.0, which is greater than 7.3 that is used for the official TF packages)</li>
<li>Clone the <a href="https://github.com/tensorflow/tensorflow">TensorFlow repository</a> from GitHub</li>
<li>Git checkout the latest official TensorFlow release (v2.2)</li>
<li>Installed the latest release of <a href="https://docs.bazel.build/versions/master/install-ubuntu.html#install-with-installer-ubuntu">Bazel</a> (Google's Make program), version 3.1. Then install exactly the right version needed for TF2.2 (2.0.0, as specified by MIN_BAZEL_VERSION in <code>tensorflow/configure.py</code>, use <code>.baselversion</code> to easily install multiple bazel versions side by side)</li>
</ul>
<p>Then came the hard part, the final step:</p>
<ul>
<li>Tweak Bazel arguments endlessly to reduce resource usage to be able to complete the build process succesfully</li>
</ul>
<p>In the end, I removed the <code>-c opt</code>, so no special optimization for my CPU. And asked for <strong>one CPU</strong> (I have two cores :-), <strong>one job</strong>, and <strong>max 2GB of RAM usage</strong>.</p>
<pre class="bash"><code>cd tf_build_env/
source bin/activate
cd ~/Github/tensorflow/
bazel build --config=opt --local_ram_resources=2048 --local_cpu_resources=HOST_CPUS-1 --jobs=1
  //tensorflow/tools/pip_package:build_pip_package</code></pre>
<p>I ran the build process in a terminal on the Ubuntu 18.04 Desktop, without any other programs loaded. My 2010 PC has in total 4 GB of RAM. As the Ubuntu Desktop + OS consumes about 1-1.5 GB on my system, this leaves about 2.5-3.0 GB for bazel. Now as it turns out, according to <code>htop</code> memory consumption went up to 3.6 GB (of my 3.9GB max), but it succeeded in the end. This was after 10 hours of compiling! (I let it run overnight)</p>
<p>The final step was to turn the compiled TensorFlow into a Python Wheel package ready to install using <code>pip</code>.</p>
<pre class="bash"><code>./bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg
# creates a &#39;wheel&#39; file called tensorflow-2.2.0-cp36-cp36m-linux_x86_64.whl</code></pre>
<p>To try it out, I created a new empty Python 3 virtual environment with only TensorFlow and Jupyter Notebook installed. To my delight it ran the <a href="https://www.tensorflow.org/tutorials/keras/classification">Fashion MNIST classification with Keras</a> example flawlessly.</p>
<p>And even on my ancient PC performance was quite good, training the model took around 1 minute. So, after glorious succes in Python, it was time to move on to R.</p>
</div>
<div id="keras-in-r-with-the-classic-mnist" class="section level1">
<h1>Keras in R with the classic MNIST</h1>
<p>I had to install the development version of the R package <code>keras</code> from GitHub to fix a bug that prevented Keras in R from working with TF v2.2.</p>
<p>From the release notes: (<a href="https://github.com/rstudio/keras/blob/master/NEWS.md" class="uri">https://github.com/rstudio/keras/blob/master/NEWS.md</a>)</p>
<p><code>Fixed issue regarding the KerasMetricsCallback with TF v2.2 (#1020)</code></p>
<pre class="r"><code>devtools::install_github(&quot;rstudio/keras&quot;)</code></pre>
<p>For my first deep learning in R, I followed the tutorial from <a href="https://tensorflow.rstudio.com/tutorials/beginners/" class="uri">https://tensorflow.rstudio.com/tutorials/beginners/</a></p>
<p>First load all the required packages.</p>
<pre class="r"><code>library(tensorflow)

use_virtualenv(&quot;~/venvs/keras_env&quot;, required = TRUE)
# this was the same environment that I tested TensorFlow with Python

library(keras)</code></pre>
<p>Read in the dataset.</p>
<pre class="r"><code>mnist &lt;- dataset_mnist()</code></pre>
<p>Rescale pixel values to be between 0 and 1.</p>
<pre class="r"><code>mnist$train$x &lt;- mnist$train$x/255
mnist$test$x &lt;- mnist$test$x/255</code></pre>
<p>Plot the data.</p>
<pre class="r"><code>x_train &lt;- mnist$train$x
y_train &lt;- mnist$train$y

# visualize the digits
par(mfcol=c(6,6))
par(mar=c(0, 0, 3, 0), xaxs=&#39;i&#39;, yaxs=&#39;i&#39;)
for (idx in 1:12) { 
    im &lt;- x_train[idx,,]
    im &lt;- t(apply(im, 2, rev)) 
    image(1:28, 1:28, im, col=gray((0:255)/255), 
          xaxt=&#39;n&#39;, main=paste(y_train[idx]))
}</code></pre>
<p><img src="/post/2020-05-15-deep_learning_keras_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
</div>
<div id="keras-model" class="section level1">
<h1>Keras model</h1>
<pre class="r"><code>model &lt;- keras_model_sequential() %&gt;% 
  layer_flatten(input_shape = c(28, 28)) %&gt;% 
  layer_dense(units = 128, activation = &quot;relu&quot;) %&gt;% 
  layer_dropout(0.2) %&gt;% 
  layer_dense(10, activation = &quot;softmax&quot;)</code></pre>
<pre class="r"><code>summary(model)</code></pre>
<pre><code>## Model: &quot;sequential&quot;
## ________________________________________________________________________________
## Layer (type)                        Output Shape                    Param #     
## ================================================================================
## flatten (Flatten)                   (None, 784)                     0           
## ________________________________________________________________________________
## dense_1 (Dense)                     (None, 128)                     100480      
## ________________________________________________________________________________
## dropout (Dropout)                   (None, 128)                     0           
## ________________________________________________________________________________
## dense (Dense)                       (None, 10)                      1290        
## ================================================================================
## Total params: 101,770
## Trainable params: 101,770
## Non-trainable params: 0
## ________________________________________________________________________________</code></pre>
<p>It has over 100.000 parameters!!</p>
<p>Python has a nice <code>plot_model()</code> function, in R we can use the <code>deepviz</code> package.</p>
<pre class="r"><code>devtools::install_github(&quot;andrie/deepviz&quot;)</code></pre>
<pre class="r"><code>library(deepviz)
library(magrittr)

model %&gt;% plot_model()</code></pre>
<pre><code>## Warning: `select_()` is deprecated as of dplyr 0.7.0.
## Please use `select()` instead.
## This warning is displayed once every 8 hours.
## Call `lifecycle::last_warnings()` to see where this warning was generated.</code></pre>
<pre><code>## Warning: The `x` argument of `as_tibble.matrix()` must have unique column names if `.name_repair` is omitted as of tibble 2.0.0.
## Using compatibility `.name_repair`.
## This warning is displayed once every 8 hours.
## Call `lifecycle::last_warnings()` to see where this warning was generated.</code></pre>
<div id="htmlwidget-1" style="width:672px;height:480px;" class="grViz html-widget"></div>
<script type="application/json" data-for="htmlwidget-1">{"x":{"diagram":"digraph {\n\ngraph [layout = \"neato\",\n       outputorder = \"edgesfirst\",\n       bgcolor = \"white\"]\n\nnode [fontname = \"Helvetica\",\n      fontsize = \"10\",\n      shape = \"circle\",\n      fixedsize = \"true\",\n      width = \"0.5\",\n      style = \"filled\",\n      fillcolor = \"aliceblue\",\n      color = \"gray70\",\n      fontcolor = \"gray50\"]\n\nedge [fontname = \"Helvetica\",\n     fontsize = \"8\",\n     len = \"1.5\",\n     color = \"gray80\",\n     arrowsize = \"0.5\"]\n\n  \"1\" [label = \"flatten\nFlatten\n\", shape = \"rectangle\", fixedsize = \"FALSE\", fillcolor = \"#F0F8FF\", fontcolor = \"#000000\", pos = \"0,4!\"] \n  \"2\" [label = \"dense_1\nDense\nrelu\", shape = \"rectangle\", fixedsize = \"FALSE\", fillcolor = \"#F0F8FF\", fontcolor = \"#000000\", pos = \"0,3!\"] \n  \"3\" [label = \"dropout\nDropout\n\", shape = \"rectangle\", fixedsize = \"FALSE\", fillcolor = \"#F0F8FF\", fontcolor = \"#000000\", pos = \"0,2!\"] \n  \"4\" [label = \"dense\nDense\nsoftmax\", shape = \"rectangle\", fixedsize = \"FALSE\", fillcolor = \"#F0F8FF\", fontcolor = \"#000000\", pos = \"0,1!\"] \n  \"1\"->\"2\" \n  \"2\"->\"3\" \n  \"3\"->\"4\" \n}","config":{"engine":"dot","options":null}},"evals":[],"jsHooks":[]}</script>
</div>
<div id="compile-the-model" class="section level1">
<h1>Compile the model</h1>
<pre class="r"><code>model %&gt;% 
  compile(
    loss = &quot;sparse_categorical_crossentropy&quot;,
    optimizer = &quot;adam&quot;,
    metrics = &quot;accuracy&quot;
  )</code></pre>
</div>
<div id="fit-the-model" class="section level1">
<h1>Fit the model</h1>
<pre class="r"><code>model %&gt;% 
  fit(
    x = mnist$train$x, 
    y = mnist$train$y,
    epochs = 5,
    validation_split = 0.3,
    verbose = 1
  )</code></pre>
</div>
<div id="make-predictions" class="section level1">
<h1>Make predictions</h1>
<pre class="r"><code>predictions &lt;- predict(model, mnist$test$x)</code></pre>
<p>Visualize a single prediction:</p>
<pre class="r"><code>library(ggplot2)

id &lt;- 9

ggplot(data.frame(digit = 0:9, prob = predictions[id,]), 
       aes(x = factor(digit), y = prob)) + geom_col() +
  ggtitle(paste0(&quot;prediction for true value of &quot;, mnist$test$y[id]))</code></pre>
<p><img src="/post/2020-05-15-deep_learning_keras_files/figure-html/unnamed-chunk-15-1.png" width="672" /></p>
</div>
<div id="check-model-performance-on-the-test-set" class="section level1">
<h1>Check model performance on the test set</h1>
<pre class="r"><code>model %&gt;% 
  evaluate(mnist$test$x, mnist$test$y, verbose = 0)</code></pre>
<pre><code>##       loss   accuracy 
## 0.08686701 0.97399998</code></pre>
<p>Our model achieved ~98% accuracy on the test set.</p>
<p>Awesome.</p>
</div>
