---
title: "Prep and fit all datasets for RF blogpost"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(AppliedPredictiveModeling)
library(caret)
library(tidyverse)
library(mlbench)
library(ranger)
```


# Storyline

Both mtry tuning and rfe work towards reducing the impact of irrelevant variables.
If a dataset contains mostly relevant variables, neither has a large impact, and RF performs well out of the box.
If a dataset contains a few relevant variables, setting mtry to a larger value might already fix things.
If the ratio between relevant and irrelevant features becomes too low, tuning mtry will no longer be enough, and rfe must be used.

```
p615 ESL
When the number of variables is large, but the fraction of relevant variables
small, random forests are likely to perform poorly with small m. At each
split the chance can be small that the relevant variables will be selected.
Figure 15.7 shows the results of a simulation that supports this claim. 
```

This storyline explains various observations:
Why people often find that rfe does not improve things (signal is strong enough)
Why people find that tuning already solves things, rfe is not necessary (increasing mtry has the same effect as reducing the irrelevant variables)
Why hastie09 finds that RF suffers from including irrelevant variables

# Outline

We compare for several datasets the optimal model after mtry tuning, and after rfe.
We also fit for default and largest mtry value.
We also fit a RF using ranger's default settings in the `caret` framework, using 10xCV.

For some datasets, there is sufficient data to set aside a true validation set.

# Read in data

## Friedman1

```{r}
source("add_spurious_predictors.R")

set.seed(1)

sim <- mlbench::mlbench.friedman1(n = 1000, sd = 1)

df_friedman1 <- data.frame(sim$x)
colnames(df_friedman1) <- paste0("X",1:10)

df_friedman1$y <- sim$y

df_friedman1_N100 <- add_spurious_predictors(df_friedman1, 100)

df_friedman1_N500 <- add_spurious_predictors(df_friedman1, 500)

rm(sim)
```

## iris


```{r}
df_iris <- iris

df_iris_N100 <- add_spurious_predictors(df_iris, 100)
```

## OpenML datasets

```{r}
# OpenML-Reg19

library(OpenML)

setOMLConfig(cachedir = "cache/")

# tecator Probst 19
tecator = getOMLDataSet(data.id = 505L, cache.only = TRUE)
tecator <- tecator$data

# bodyfat Probst 2
bodyfat = getOMLDataSet(data.id = 560L, cache.only = TRUE)
bodyfat <- bodyfat$data

# puma32H Probst 28
puma32H = getOMLDataSet(data.id = 308L, cache.only = TRUE)
puma32H <- puma32H$data
```

`bodyfat` op OpenML heeft een rare dep var, terwijl de Kaggle versie een depvar "Body fat" heeft.

```{r}
plot(bodyfat$class, bodyfat$Density)
```



### Pumadyn: a classic ML dataset for a simulated PUMA 560 robotarm

![](puma560_schematic.png)

It appears that this robot arm was popular in the 1980s, and was used a lot in ML research during the 1990s.
The dataset contains 32 predictors, and the task is to predict the angular acceleration of one of the robot arms links.

```{r}
# 32nm = 32 inputs, high nonlinearity, med noise
# Further details of the Pumadyn data sets can be found in Ghahramani
ggplot(puma32H, aes(x = tau4, y = thetadd6, color = theta5)) + geom_point() +
  scale_color_gradient2() + facet_wrap(~ round(theta5)) +
  ggtitle("thetadd6 by tau4 and theta5")
```

 
## ISLR datasets (NCI60)

Genomic classification

```{r}
library(ISLR)
NCI60 <- ISLR::NCI60
df_NCI60 <- data.frame(CLASS = NCI60$labs, NCI60$data, row.names = NULL)

cell_lines <- c("CNS", "LEUKEMIA", "OVARIAN", "BREAST", "COLON", "MELANOMA",  "NSCLC", "RENAL")

df_NCI60 <- df_NCI60 %>% 
  filter(CLASS %in% cell_lines)

df_NCI60$CLASS <- factor(df_NCI60$CLASS)
```

## Caret datasets (solubility)

```{r}
library(AppliedPredictiveModeling)

data(solubility)
rm(solTrainX, solTestX)

solubility_test <- data.frame(cbind(y = solTestY, solTestXtrans))
solubility <- data.frame(cbind(y = solTrainY, solTrainXtrans))

solubility_N100 <- add_spurious_predictors(solubility, 100)
solubility_N500 <- add_spurious_predictors(solubility, 500)
```

```{r}
# shuffle Y value, destroy all signal
solubility_N500_perm <- solubility_N500

set.seed(1234)

n_rows <- length(solTrainY)
solubility_N500_perm$y <- solTrainY[sample(1:n_rows, size = n_rows, replace = F)]

rm(solTestY, solTestXtrans)
rm(solTrainY, solTrainXtrans)
```




# Gather datasets

So far, we have:

* Friedman1 (source: caret)
* Friedman1+100noise 
* Friedman1+500noise 

* iris (source: M Wright)
* iris+100noise 

* NCI60 (ISLR cancer cell lines)

* solubility (source: APM)
* solubility+100noise (source: APM)
* solubility+500noise (source: APM)
* solubility+500noise_Ypermutated

* tecator (source: OpenML / Probst tuneRanger)
* bodyfat (source: OpenML / Probst tuneRanger)
* puma32H (source: OpenML / Probst tuneRanger)

The final three datasets were selected from results by Probst. There are all three datasets where tuning mtry has a large effect on R2.

```{r}
ds_name <- c("df_friedman1", 
             "df_friedman1_N100", 
             "df_friedman1_N500",
             "df_iris",
             "df_iris_N100",
             "df_NCI60",
             "puma32H",
             "tecator",
             "bodyfat",
             "solubility",
             "solubility_N100",
             "solubility_N500",
             "solubility_N500_perm"
             )

ds_group <- c("df_friedman1", 
             "df_friedman1", 
             "df_friedman1",
             "df_iris",
             "df_iris",
             "df_NCI60",
             "openML",
             "openML",
             "openML",
             "solubility",
             "solubility",
             "solubility",
             "solubility"
             )

ds_target <- c("y",
               "y",
               "y",
               "Species",
               "Species",
               "CLASS",
               "thetadd6",
               "fat",
               "class",
               "y",
               "y",
               "y",
               "y"
             )

ds_forest_type <- c("reg",
               "reg",
               "reg",
               "class",
               "class",
               "class",
               "reg",
               "reg",
               "reg",
               "reg",
               "reg",
               "reg",
               "reg"
             )


if(exists("ds_list")) rm(ds_list)

for(i in 1:length(ds_name)){
  name <- ds_name[i]
  ds_list_tmp <- data.frame(ds_id = i,
                            ds_name = name,
                            ds_group = ds_group[i],
                            target = ds_target[i],
                            forest_type = ds_forest_type[i],
                            n_obs = nrow(get(name)),
                            n_features = ncol(get(name)) - 1
  )
  if(exists("ds_list")){
    ds_list <- rbind(ds_list, ds_list_tmp)
  } else {ds_list <- ds_list_tmp}
}
```

```{r}
knitr::kable(ds_list)
```

# Model list

We want ranger fits for both default and max mtry.

```{r}
model_list <- data.frame(model_id = c(1:3),
                         method = c("ranger", "ranger", "ranger-rfe"), 
                         mtry = c("default", "max", "default"))

model_list
```
`max` is bagging setting, i.e. offer all predictors.

```{r}
fit_list <- crossing(model_list, ds_list)

fit_list$fit_id <- 1:nrow(fit_list)
```

```{r}
fit_list <- fit_list %>% 
  left_join(data.frame(ds_id = 1:13, ds_group)) %>% 
  arrange(ds_group) #%>% filter(ds_group == "df_friedman1")
```

```{r}
fit_list
```

# Fit all methods on all datasets (CV performance)

```{r}
source("rangerFuncs.R")

ctrl <- rfeControl(functions = rangerFuncs,
                   method = "cv",
                   returnResamp = "all",
                   verbose = TRUE,
                   saveDetails = TRUE)
```


```{r}
res <- list()
fit_list$performance <- NA
dir.create("fits")
fullrun <- 0

if(fullrun){
  for (i in 1:nrow(fit_list)){
    print(".")
    
    formula <- as.formula(paste0(fit_list[i,]$target, " ~ ."))
    
    if(fit_list[i,]$method == "ranger"){
        if(fit_list[i,]$mtry == "max"){
          mtry = fit_list[i,]$n_features
        } else {
          mtry = floor(sqrt(fit_list[i,]$n_features))
        }
        set.seed(123)
        fit <- ranger(formula, 
                      data = get(fit_list[i,]$ds_name),
                      mtry = mtry,
                      probability = FALSE, #fit_list[i,]$forest_type == "class",
                      num.threads = 6 )
        
        filename <- paste0("fits/fit_", fit_list[i,]$fit_id, ".rds")
        saveRDS(object = fit, file = filename)
        
        fit_list[i,]$performance <- ifelse(fit_list[i,]$forest_type == "reg", 
                                           fit$r.squared,
                                           1 - fit$prediction.error)
    }
    if(fit_list[i,]$method == "ranger-rfe"){
      
      if(fit_list[i,]$forest_type == "reg"){ 
        perf_metric =  "Rsquared" 
        } else {perf_metric =  "Accuracy"}
 
      subsets <- caret:::var_seq(fit_list[i,]$n_features, len = 10)

      set.seed(123)
      
      rfe_fit <- rfe(formula, 
                 data = get(fit_list[i,]$ds_name),
                 metric = perf_metric,
                 sizes = subsets, 
                 rfeControl = ctrl,
                 num.threads = 6)
        
        filename <- paste0("fits/fit_", fit_list[i,]$fit_id, ".rds")
        saveRDS(object = rfe_fit, file = filename)
        
        fit_list[i,]$performance <- ifelse(fit_list[i,]$forest_type == "reg", 
                                           rfe_fit$fit$r.squared,
                                           1 - rfe_fit$fit$prediction.error)
    }
    
  }
  
  filename <- paste0("fitlist.rds")
  saveRDS(object = fit_list, file = filename)
} else {
  filename <- paste0("fitlist.rds")
  fit_list <- readRDS(filename) 
  fit_list$r_sq <- NULL}

fit_list$algo <- paste0(fit_list$method, "_mtry_", fit_list$mtry)
```



## Ranger separate calls 

### Friedman1 datasets




```{r}
ggplot(fit_list %>% filter(ds_group == "df_friedman1"), aes(x = reorder(algo, performance), y = performance, col = factor(algo))) +
  geom_point() +
  geom_label(aes(label = round(performance,2)), nudge_y = -0.2, nudge_x = +0.1) +
  geom_segment(aes(xend = algo, y = 0, yend = performance)) +
  coord_flip() +
  facet_wrap(~ ds_name)
```

It looks like tuning mtry can reduce the negative effect of irrelevant features, but not completely eliminate it as RFE can do. The subsets are important here: N100 has 16 vars (5 signal, rest noise), N500 has 6 vars.

```{r}
fit_list %>% filter(ds_group == "df_friedman1")
```

```{r}
rfe_fit <- readRDS("fits/fit_29.rds")

rfe_fit$fit
```



### ISLR datasets (NCI60)

RFE uses probability (multi class) and Brier score, whereas ranger checks majority vote and used prediction error.

```{r}
ggplot(fit_list %>% filter(ds_group == "df_NCI60"), aes(x = reorder(algo, performance), y = performance, col = factor(algo))) +
  geom_point() +
  geom_label(aes(label = round(performance,2)), nudge_y = -0.2, nudge_x = +0.1) +
  geom_segment(aes(xend = algo, y = 0, yend = performance)) +
  coord_flip() +
  facet_wrap(~ ds_name)
```

```{r}
rf <- ranger(CLASS ~ ., probability = FALSE, data = df_NCI60)


table(df_NCI60$CLASS, rf$predictions)

rf
```

Here RFE has not found the "sweet spot", which is weird because the full dataset and default mtry should also be part of the settings.

```{r}
fit_list %>% filter(ds_group == "df_NCI60")

rfe_fit <- readRDS("fits/fit_32.rds")

rfe_fit$fit
```
```{r}
rfe_fit
```


### OpenML datasets

https://topepo.github.io/caret/data-sets.html#tecator-nir-data

For the three OpenML datasets, mtry tuning is sufficient to get optimal performance.

```{r}
ggplot(fit_list %>% filter(ds_group == "openML"), aes(x = reorder(algo, performance), y = performance, col = factor(algo))) +
  geom_point() +
  geom_label(aes(label = round(performance,2)), nudge_y = -0.2, nudge_x = +0.1) +
  geom_segment(aes(xend = algo, y = 0, yend = performance)) +
  coord_flip() +
  facet_wrap(~ ds_name)
```


## caret with mtry tuning (CV / OOB)

### iris datasets

```{r}
ggplot(fit_list %>% filter(ds_group == "df_iris"), aes(x = reorder(algo, performance), y = performance, col = factor(algo))) +
  geom_point() +
  geom_label(aes(label = round(performance,2)), nudge_y = -0.02, nudge_x = +0.1) +
  geom_segment(aes(xend = algo, y = 0, yend = performance)) +
  coord_flip() +
  facet_wrap(~ ds_name)
```


```{r}
set.seed(123)
rangerGrid <- data.frame(mtry = 2:4,
                       splitrule = "gini", 
                       min.node.size = 10)

res <- train(Species ~ ., 
             data = df_iris, 
             method = "ranger", 
             tuneGrid = rangerGrid,            
             trControl = trainControl(method = "cv"))
res
```

```{r}
rangerGrid <- data.frame(mtry = c(2:4,40,80,104),
                       splitrule = "gini", 
                       min.node.size = 10)

res <- train(Species ~ ., 
             data = df_iris_N100, 
             method = "ranger", 
             tuneGrid = rangerGrid,
             trControl = trainControl(method = "cv"),
             num.threads = 6)
res
```

### Friedman1 datasets


```{r}
ggplot(fit_list %>% filter(ds_group == "df_friedman1"), aes(x = reorder(algo, performance), y = performance, col = factor(algo))) +
  geom_point() +
  geom_label(aes(label = round(performance,2)), nudge_y = -0.02, nudge_x = +0.1) +
  geom_segment(aes(xend = algo, y = 0, yend = performance)) +
  coord_flip() +
  facet_wrap(~ ds_name)
```

```{r}
set.seed(1234)

rangerGrid <- data.frame(mtry = c(2:5),
                       splitrule = "variance", 
                       min.node.size = 5)

res <- train(y ~ X1 + X2 + X3 + X4 + X5, 
              data = df_friedman1, 
              method = "ranger", 
              tuneGrid = rangerGrid,
              trControl = trainControl(method = "oob")
             )

res 
```

If we tune on the DGP variables, we get the best possible performance.


```{r}
set.seed(1234)

rangerGrid <- data.frame(mtry = c(2:5, 30, 60, 110),
                       splitrule = "variance", 
                       min.node.size = 5)

res <- train(y ~ ., 
              data = df_friedman1_N100, 
              method = "ranger", 
              metric = "Rsquared",
              tuneGrid = rangerGrid,
              trControl = trainControl(method = "oob"),
              num.threads = 6
             )

plot(res)
```

```{r}
res
```
Simply tuning the RF does not give optimal performance.

```{r}
set.seed(1234)

rangerGrid <- data.frame(mtry = c(5, 60, 110, 160, 220, 510),
                       splitrule = "variance", 
                       min.node.size = 10)

res <- train(y ~ ., 
              data = df_friedman1_N500, 
              method = "ranger", 
              metric = "Rsquared",
              tuneGrid = rangerGrid,
              trControl = trainControl(method = "oob"),
             num.threads = 6
             )

plot(res)
```

This gets worse if we add more noise variables.

### OpenML datasets

```{r}
ggplot(fit_list %>% filter(ds_group == "openML"), aes(x = reorder(algo, performance), y = performance, col = factor(algo))) +
  geom_point() +
  geom_label(aes(label = round(performance,2)), nudge_y = -0.02, nudge_x = +0.1) +
  geom_segment(aes(xend = algo, y = 0, yend = performance)) +
  coord_flip() +
  facet_wrap(~ ds_name)
```

```{r}
set.seed(1234)

rangerGrid <- data.frame(mtry = c(2,11, 63,124),
                       splitrule = "variance",
                       min.node.size = 5)

res <- train(fat ~ ., 
              data = tecator, 
              method = "ranger", 
              metric = "Rsquared",
              tuneGrid = rangerGrid,
              trControl = trainControl(method = "oob"),
             num.threads = 6
             )

res
```

Here tuning mtry gives us optimal performance.


```{r}
set.seed(1234)

rangerGrid <- data.frame(mtry = c(2,11, 32),
                       splitrule = "variance",
                       min.node.size = 5)

res <- train(thetadd6 ~ ., 
              data = puma32H, 
              method = "ranger", 
              metric = "Rsquared",
              tuneGrid = rangerGrid,
              trControl = trainControl(method = "oob"),
             num.threads = 6
             )

res
```

Here tuning mtry gives us optimal performance.

## rfe - ranger

Common settings:
```{r}
source("rangerFuncs.R")

ctrl <- rfeControl(functions = rangerFuncs,
                   method = "cv",
                   returnResamp = "all",
                   verbose = TRUE,
                   saveDetails = TRUE)
```

### ISLR NCI60


Leave this out of blog, too high variance / uncertainty on the performance metric.

```{r}
ggplot(fit_list %>% filter(ds_group == "df_NCI60"), aes(x = reorder(algo, performance), y = performance, col = factor(algo))) +
  geom_point() +
  geom_label(aes(label = round(performance,2)), nudge_y = -0.02, nudge_x = +0.1) +
  geom_segment(aes(xend = algo, y = 0, yend = performance)) +
  coord_flip() +
  facet_wrap(~ ds_name)
```
low n, high p, genetic marker dataset.

```{r}
#df_NCI60

subsets <- c(10,75,100) # 100,500

set.seed(10)
rfProfile <- rfe(CLASS ~ ., 
                 data = df_NCI60, 
                 metric = "Accuracy",
                 sizes = subsets, 
                 rfeControl = ctrl,
                 num.threads = 6)

rfProfile
```
RFE does not improve. Chooses the model with all predictors.

```{r}
rfProfile$fit
```

The SD is huge here: a rerun gives the highest accuracy to the untuned RF with default mtry.

### Friedman1 datasets

```{r}
subsets <- c(1,4,5,6)

set.seed(10)
rfProfile <- rfe(y ~ ., 
                 data = df_friedman1_N100, 
                 metric = "Rsquared",
                 sizes = subsets, 
                 rfeControl = ctrl,
                 num.threads = 6)

plot(rfProfile)

```
Here, RFE beats mtry tuning. Can we also find a non artificial dataset where this holds?

```{r}
rfProfile
```


### OpenML datasets

```{r}
#subsets <- c(1:10, 150, 200, 300)
subsets <- c(1:5)

set.seed(10)
rfProfile <- rfe(thetadd6 ~ ., 
                 data = puma32H, 
                 metric = "Rsquared",
                 sizes = subsets, 
                 rfeControl = ctrl,
                 num.threads = 6)

plot(rfProfile)
```

```{r}
rfProfile
```


### Solubility models


The APM dataset solubility shows all three methods giving similar results in the absence of noise.
After adding noise, tuning mtry helps, but is not enough.
Again, this is a artificial dataset with a large amount of noise.
The situation gets worse if we add more noise.

As a check that we do not fit on noise, after shuffling the Y variable all three methods produce an R2 of zero.

We can compare this with refitting after the RFE variable selection.

```{r}
ggplot(fit_list %>% filter(ds_group == "solubility"), aes(x = reorder(algo, performance), y = performance, col = factor(algo))) +
  geom_point() +
  geom_label(aes(label = round(performance,2)), nudge_y = -0.05, nudge_x = +0.25) +
  geom_segment(aes(xend = algo, y = 0, yend = performance)) +
  coord_flip() +
  facet_wrap(~ ds_name)
```

### refit models after RFE: this risks selection bias 

```{r}
ranger(fat ~ moisture + protein, 
       data = tecator,
       num.threads = 6)
```

```{r}
ranger(class ~ Density + Abdomen, 
       data = bodyfat,
       num.threads = 6)
```

```{r}
ranger(thetadd6 ~ tau4 + theta5, 
       data = puma32H,
       num.threads = 6)
```

OOB error is identical to RFE/mtry results. Nothing special here.

## rfe - ranger with mtry tuning

Conclusion: although it is technically possible, it has no use case.

```{r}
# source("rangerTuneFuncs.R")
```


## refitting after rfe: solubility permuted


```{r}
source("rangerFuncs.R")

ctrl <- rfeControl(functions = rangerFuncs,
                   method = "cv",
                   returnResamp = "all",
                   verbose = TRUE,
                   saveDetails = TRUE)
```


```{r}
#fit_list %>% filter(method == "ranger-rfe")

#fit <- readRDS("fits/fit_39.rds")

# RFE subsets to use during backward feature selection
set.seed(123)

subsets <- round(exp(seq(log(1), log(ncol(solubility_N500_perm)-1), 
                         length.out = sqrt(ncol(solubility_N500_perm)-1))))

fullrun <- 0

if(fullrun){
  fit <- rfe(y ~., 
             data = solubility_N500_perm,
             sizes = c(35,75,100),
             metric = "RMSE",
             rfeControl = ctrl,
             num.threads = 6)
  
  saveRDS(fit, "rfe_fit.rds")
} else {
  fit <- readRDS("rfe_fit.rds")
}

fit
```


```{r}
# subset on selected features
preds <- predictors(fit)
Xrfe <- solubility_N500_perm %>% select(all_of(preds))

set.seed(123)

# check mtry at different orders of magnitude
tune_grid <- expand.grid(mtry = round(exp(seq(log(1), log(length(preds)), length.out=sqrt(length(preds))))),
                         splitrule = "variance",
                         min.node.size = 5)

fullrun <- 0

if(fullrun) {
  trainObject <- train(x = Xrfe,
             y = solubility_N500_perm$y,
             method = "ranger",
             trControl = trainControl(method = "cv"),
             tuneGrid = tune_grid,
             num.threads = 6)
  saveRDS(trainObject, "post_rfe_train.rds")
} else {
  trainObject <- readRDS("post_rfe_train.rds")
}
```

```{r}
trainObject
```




