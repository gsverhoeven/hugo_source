---
title: "Prep and fit all datasets for RF blogpost"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(AppliedPredictiveModeling)
library(caret)
library(tidyverse)
library(mlbench)
library(ranger)
```


# Storyline

Both mtry tuning and rfe work towards reducing the impact of irrelevant variables.
If a dataset contains mostly relevant variables, neither has a large impact, and RF performs well out of the box.
If a dataset contains a few relevant variables, setting mtry to a larger value might already fix things.
If the ratio between relevant and irrelevant features becomes too low, tuning mtry will no longer be enough, and rfe must be used.

```
p615 ESL
When the number of variables is large, but the fraction of relevant variables
small, random forests are likely to perform poorly with small m. At each
split the chance can be small that the relevant variables will be selected.
Figure 15.7 shows the results of a simulation that supports this claim. 
```

This storyline explains various observations:
Why people often find that rfe does not improve things (signal is strong enough)
Why people find that tuning already solves things, rfe is not necessary (increasing mtry has the same effect as reducing the irrelevant variables)
Why hastie09 finds that RF suffers from including irrelevant variables

# Outline

We compare for several datasets the optimal model after mtry tuning, and after rfe.
We also fit for default and largest mtry value.
We also fit a RF using ranger's default settings in the `caret` framework, using 10xCV.

For some datasets, there is sufficient data to set aside a true validation set.

# Read in data

## Friedman1

```{r}
source("add_spurious_predictors.R")

set.seed(1)

sim <- mlbench::mlbench.friedman1(n = 1000, sd = 1)

df_friedman1 <- data.frame(sim$x)
colnames(df_friedman1) <- paste0("X",1:10)

df_friedman1$y <- sim$y

df_friedman1_N100 <- add_spurious_predictors(df_friedman1, 100)

df_friedman1_N500 <- add_spurious_predictors(df_friedman1, 500)

rm(sim)
```

## iris
```{r}
df_iris <- iris

df_iris_N100 <- add_spurious_predictors(df_iris, 100)
```

## OpenML datasets

```{r}
# OpenML-Reg19

library(OpenML)

setOMLConfig(cachedir = "cache/")

# tecator Probst 19
tecator = getOMLDataSet(data.id = 505L)
tecator <- tecator$data

# bodyfat Probst 2
bodyfat = getOMLDataSet(data.id = 560L)
bodyfat <- bodyfat$data

# puma32H Probst 28
puma32H = getOMLDataSet(data.id = 308L)
puma32H <- puma32H$data
```

## Pumadyn: a classic ML dataset for a simulated PUMA 560 robotarm

![](puma560_schematic.png)

It appears that this robot arm was popular in the 1980s, and was used a lot in ML research during the 1990s.
The dataset contains 32 predictors, and the task is to predict the angular acceleration of one of the robot arms links.

```{r}
# 32nm = 32 inputs, high nonlinearity, med noise
# Further details of the Pumadyn data sets can be found in Ghahramani
ggplot(puma32H, aes(x = tau4, y = thetadd6, color = theta5)) + geom_point() +
  scale_color_gradient2() + facet_wrap(~ round(theta5)) +
  ggtitle("thetadd6 by tau4 and theta5")
```


## ISLR datasets

Genomic classification

```{r}
library(ISLR)
NCI60 <- ISLR::NCI60
df_NCI60 <- data.frame(CLASS = NCI60$labs, NCI60$data, row.names = NULL)

cell_lines <- c("CNS", "LEUKEMIA", "OVARIAN", "BREAST", "COLON", "MELANOMA",  "NSCLC", "RENAL")

df_NCI60 <- df_NCI60 %>% 
  filter(CLASS %in% cell_lines)

df_NCI60$CLASS <- factor(df_NCI60$CLASS)
```

## Caret datasets (solubility)

```{r}
library(AppliedPredictiveModeling)

data(solubility)
rm(solTrainX, solTestX)

solubility_test <- data.frame(cbind(y = solTestY, solTestXtrans))
solubility <- data.frame(cbind(y = solTrainY, solTrainXtrans))

solubility_N100 <- add_spurious_predictors(solubility, 100)
solubility_N500 <- add_spurious_predictors(solubility, 500)
```

```{r}
# shuffle Y value, destroy all signal
solubility_N500_perm <- solubility_N500

set.seed(1234)

n_rows <- length(solTrainY)
solubility_N500_perm$y <- solTrainY[sample(1:n_rows, size = n_rows, replace = F)]

rm(solTestY, solTestXtrans)
rm(solTrainY, solTrainXtrans)
```




# Gather datasets

So far, we have:

* Friedman1 (source: caret)
* Friedman1+100noise 
* Friedman1+500noise 

* iris (source: M Wright)
* iris+100noise 

* NCI60 (ISLR cancer cell lines)

* solubility (source: APM)
* solubility+100noise (source: APM)
* solubility+500noise (source: APM)
* solubility+500noise_Ypermutated

* tecator (source: OpenML / Probst tuneRanger)
* bodyfat (source: OpenML / Probst tuneRanger)
* puma32H (source: OpenML / Probst tuneRanger)

The final three datasets were selected from results by Probst. There are all three datasets where tuning mtry has a large effect on R2.

```{r}
ds_name <- c("df_friedman1", 
             "df_friedman1_N100", 
             "df_friedman1_N500",
             "df_iris",
             "df_iris_N100",
             "df_NCI60",
             "puma32H",
             "tecator",
             "bodyfat",
             "solubility",
             "solubility_N100",
             "solubility_N500",
             "solubility_N500_perm"
             )

ds_target <- c("y",
               "y",
               "y",
               "Species",
               "Species",
               "CLASS",
               "thetadd6",
               "fat",
               "class",
               "y",
               "y",
               "y",
               "y"
             )


if(exists("ds_list")) rm(ds_list)
for(i in 1:length(ds_name)){
  name <- ds_name[i]
  ds_list_tmp <- data.frame(ds_id = i,
                            ds_name = name,
                            target = ds_target[i],
                            n_obs = nrow(get(name)),
                            n_features = ncol(get(name)) - 1
  )
  if(exists("ds_list")){
    ds_list <- rbind(ds_list, ds_list_tmp)
  } else {ds_list <- ds_list_tmp}
}
```

```{r}
knitr::kable(ds_list)
```

# Model list

We want ranger fits for both default and max mtry.

```{r}
model_list <- data.frame(model_id = c(1:2),
                         method = "ranger", 
                         mtry = c("default", "max"))

model_list
```
`max` is bagging setting, i.e. offer all predictors.

```{r}
fit_list <- crossing(model_list, ds_list)

fit_list$fit_id <- 1:nrow(fit_list)
```

```{r}
fit_list
```

## ranger (OOB performance)

```{r}
res <- list()
fit_list$r_sq <- NA
dir.create("fits")

for (i in 1:nrow(fit_list)){
  if(i > 2) break
  print(".")
  formula <- as.formula(paste0(fit_list[i,]$target, " ~ ."))
  if(fit_list[i,]$method == "ranger"){
      if(fit_list[i,]$mtry == "max"){
        mtry = fit_list[i,]$n_features
      } else {
        mtry = floor(sqrt(fit_list[i,]$n_features))
      }

      fit <- ranger(formula, 
                    data = get(fit_list[i,]$ds_name),
                    mtry = mtry)
      
      filename <- paste0("fits/fit_", fit_list[i,]$fit_id, ".rds")
      saveRDS(object = fit, file = filename)
      
      fit_list[i,]$r_sq <- fit$r.squared
  }
}

filename <- paste0("fits/fitlist.rds")
saveRDS(object = fit_list, file = filename)
```

## Ranger separate calls 

### Friedman1 datasets
```{r}
ranger(y ~ ., data = df_friedman1)
ranger(y ~ ., data = df_friedman1, mtry = 10)
ranger(y ~ ., data = df_friedman1_N100)
ranger(y ~ ., data = df_friedman1_N100, mtry = 110)
ranger(y ~ ., data = df_friedman1_N500)
ranger(y ~ ., data = df_friedman1_N500, mtry = 510)
```
It looks like tuning mtry can reduce the negative effect of irrelevant features, but not completely eliminate it as RFE can do.

### ISLR datasets

```{r}
rf <- ranger(CLASS ~ ., data = df_NCI60)


table(df_NCI60$CLASS, rf$predictions)
```

### OpenML datasets

https://topepo.github.io/caret/data-sets.html#tecator-nir-data

```{r}
ranger(fat ~ ., data = tecator$data)
ranger(fat ~ ., data = tecator$data, mtry = 124) 

ranger(class ~ ., data = bodyfat$data)
ranger(class ~ ., data = bodyfat$data, mtry = 14)

ranger(thetadd6 ~ ., data = puma32H$data)
ranger(thetadd6 ~ ., data = puma32H$data, mtry = 32)
```
For the three OpenML datasets, mtry tuning is sufficient to get optimal performance.


## caret with mtry tuning (CV / OOB)

### iris datasets

```{r}
set.seed(123)
rangerGrid <- data.frame(mtry = 2:4,
                       splitrule = "gini", 
                       min.node.size = 10)

res <- train(Species ~ ., 
             data = df_iris, 
             method = "ranger", 
             tuneGrid = rangerGrid,            
             trControl = trainControl(method = "cv"))
res
```

```{r}
rangerGrid <- data.frame(mtry = c(2:4,40,80,104),
                       splitrule = "gini", 
                       min.node.size = 10)

res <- train(Species ~ ., 
             data = df_iris_N100, 
             method = "ranger", 
             tuneGrid = rangerGrid,
             trControl = trainControl(method = "cv"))
res
```

### Friedman1 datasets

```{r}
set.seed(1234)

rangerGrid <- data.frame(mtry = c(2:5),
                       splitrule = "variance", 
                       min.node.size = 5)

res <- train(y ~ X1 + X2 + X3 + X4 + X5, 
              data = df_friedman1, 
              method = "ranger", 
              tuneGrid = rangerGrid,
              trControl = trainControl(method = "oob")
             )

res 
```


```{r}
set.seed(1234)

rangerGrid <- data.frame(mtry = c(2:5, 30, 60, 110),
                       splitrule = "variance", 
                       min.node.size = 5)

res <- train(y ~ ., 
              data = df_friedman1_N100, 
              method = "ranger", 
              metric = "Rsquared",
              tuneGrid = rangerGrid,
              trControl = trainControl(method = "oob")
             )

plot(res)
```

```{r}
set.seed(1234)

rangerGrid <- data.frame(mtry = c(5, 60, 110, 160, 220, 510),
                       splitrule = "variance", 
                       min.node.size = 10)

res <- train(y ~ ., 
              data = df_friedman1_N500, 
              method = "ranger", 
              metric = "Rsquared",
              tuneGrid = rangerGrid,
              trControl = trainControl(method = "oob")
             )

plot(res)
```
### OpenML datasets

```{r}
set.seed(1234)

rangerGrid <- data.frame(mtry = c(2,11, 63,124),
                       splitrule = "variance",
                       min.node.size = 5)

res <- train(fat ~ ., 
              data = tecator$data, 
              method = "ranger", 
              metric = "Rsquared",
              tuneGrid = rangerGrid,
              trControl = trainControl(method = "oob")
             )

plot(res)
```

## rfe - ranger

Common settings:
```{r}
source("rangerFuncs.R")

ctrl <- rfeControl(functions = rangerFuncs,
                   method = "cv",
                   returnResamp = "all",
                   verbose = TRUE,
                   saveDetails = TRUE)
```

### ISLR NCI60

```{r}
#df_NCI60

subsets <- c(1,10,100,500)

set.seed(10)
rfProfile <- rfe(CLASS ~ ., 
                 data = df_NCI60, 
                 metric = "Accuracy",
                 sizes = subsets, 
                 rfeControl = ctrl)

plot(rfProfile)
```
```{r}
rfProfile
```


### Friedman1 datasets

```{r}
subsets <- c(1,4,5,6)

set.seed(10)
rfProfile <- rfe(y ~ ., 
                 data = df_friedman1_N100, 
                 metric = "Rsquared",
                 sizes = subsets, 
                 rfeControl = ctrl)

plot(rfProfile)

# Error in { : task 1 failed - "undefined columns selected"
```
Here, RFE beats mtry tuning. Can we also find a non artificial dataset where this holds?

```{r}
rfProfile
```


### OpenML datasets

```{r}
#subsets <- c(1:10, 150, 200, 300)
subsets <- c(1:5)

set.seed(10)
rfProfile <- rfe(thetadd6 ~ ., 
                 data = puma32H, 
                 metric = "Rsquared",
                 sizes = subsets, 
                 rfeControl = ctrl)

plot(rfProfile)
```

### refit models after RFE: this risks selection bias 

```{r}
ranger(fat ~ moisture + protein, 
       data = tecator$data)
```

```{r}
ranger(class ~ Density + Abdomen, 
       data = bodyfat$data)
```

```{r}
ranger(thetadd6 ~ tau4 + theta5, 
       data = puma32H$data)
```

## rfe - ranger with mtry tuning

Conclusion: although it is technically possible, it has no use case.

```{r}
rfProfile
```
