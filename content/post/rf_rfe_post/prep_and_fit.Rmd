---
title: "Prep and fit all datasets for RF blogpost"
author: "Gertjan Verhoeven"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(AppliedPredictiveModeling)
library(caret)
library(tidyverse)
library(mlbench)
library(ranger)
```

# Read and prep datasets

## Friedman1

```{r}
source("add_spurious_predictors.R")

set.seed(1)

sim <- mlbench::mlbench.friedman1(n = 1000, sd = 1)

df_friedman1 <- data.frame(sim$x)
colnames(df_friedman1) <- paste0("X",1:10)

df_friedman1$y <- sim$y

df_friedman1_N100 <- add_spurious_predictors(df_friedman1, 100)

df_friedman1_N500 <- add_spurious_predictors(df_friedman1, 500)

rm(sim)
```

```{r}
n <- 100
p <- 40
sigma <- 1
set.seed(1)
sim <- mlbench.friedman1(n, sd = sigma)
colnames(sim$x) <- c(paste("real", 1:5, sep = ""),
                     paste("bogus", 1:5, sep = ""))
bogus <- matrix(rnorm(n * p), nrow = n)
colnames(bogus) <- paste("bogus", 5+(1:ncol(bogus)), sep = "")
x <- cbind(sim$x, bogus)
y <- sim$y

df_friedman1_small <- data.frame(y, x)
```

```{r}
ranger(y ~ ., df_friedman1_small, mtry = 50)
```

## iris


```{r}
df_iris <- iris

df_iris_N100 <- add_spurious_predictors(df_iris, 100)
```

## OpenML datasets

Source: `OpenML-Reg19` collection of datasets.

```{r}
library(OpenML)

setOMLConfig(cachedir = "cache/")
```

### Tecator

```{r}
# tecator Probst 19
tecator = getOMLDataSet(data.id = 505L, cache.only = TRUE)
tecator <- tecator$data
```

### Body fat

```{r}
# bodyfat Probst 2
bodyfat = getOMLDataSet(data.id = 560L)
bodyfat <- bodyfat$data
```

`bodyfat` on OpenML has a weird dependent variable ("class"), whereas the same dataset on Kaggle has "Body fat" as a dependent variable, which is also present on OpenML. Both variables correlate strongly, giving us an artificial situation with one predictor being highly predictive of the outcome, with the other variables demoted to the status of noise variable.

```{r}
plot(bodyfat$class, bodyfat$Density)
```

### Pumadyn: a classic ML dataset for a simulated PUMA 560 robotarm

```{r}
# puma32H Probst 28
puma32H = getOMLDataSet(data.id = 308L, cache.only = TRUE)
puma32H <- puma32H$data
```


![](puma560_schematic.png)

It appears that this robot arm was popular in the 1980s, and was used a lot in ML research during the 1990s.
The dataset contains 32 predictors, and the task is to predict the angular acceleration of one of the robot arms links.

```{r}
# 32nm = 32 inputs, high nonlinearity, med noise
# Further details of the Pumadyn data sets can be found in Ghahramani
ggplot(puma32H, aes(x = tau4, y = thetadd6, color = theta5)) + geom_point() +
  scale_color_gradient2() + facet_wrap(~ round(theta5)) +
  ggtitle("thetadd6 by tau4 and theta5")
```

 
## ISLR datasets (NCI60)

Genomic classification

```{r}
library(ISLR)
NCI60 <- ISLR::NCI60
df_NCI60 <- data.frame(CLASS = NCI60$labs, NCI60$data, row.names = NULL)

cell_lines <- c("CNS", "LEUKEMIA", "OVARIAN", "BREAST", "COLON", "MELANOMA",  "NSCLC", "RENAL")

df_NCI60 <- df_NCI60 %>% 
  filter(CLASS %in% cell_lines)

df_NCI60$CLASS <- factor(df_NCI60$CLASS)
```

## Caret datasets (solubility)

```{r}
library(AppliedPredictiveModeling)

data(solubility)
rm(solTrainX, solTestX)

solubility_test <- data.frame(cbind(y = solTestY, solTestXtrans))
solubility <- data.frame(cbind(y = solTrainY, solTrainXtrans))

solubility_N100 <- add_spurious_predictors(solubility, 100)
solubility_N500 <- add_spurious_predictors(solubility, 500)
```

```{r}
# shuffle Y value, destroy all signal
solubility_N500_perm <- solubility_N500

set.seed(1234)

n_rows <- length(solTrainY)
solubility_N500_perm$y <- solTrainY[sample(1:n_rows, size = n_rows, replace = F)]

rm(solTestY, solTestXtrans)
rm(solTrainY, solTrainXtrans)
```

## Cox2


```{r}
data(cox2)

cox2_class <- data.frame(class = cox2Class, cox2Descr)
```

```{r}
ranger(class ~ ., mtry = 255, data = cox2_class)

```

# Gather datasets

So far, we have:

* Friedman1 (source: caret)
* Friedman1+100noise 
* Friedman1+500noise 

* iris (source: M Wright)
* iris+100noise 

* NCI60 (ISLR cancer cell lines)

* solubility (source: APM)
* solubility+100noise (source: APM)
* solubility+500noise (source: APM)
* solubility+500noise_Ypermutated

* tecator (source: OpenML / Probst tuneRanger)
* bodyfat (source: OpenML / Probst tuneRanger)
* puma32H (source: OpenML / Probst tuneRanger)

The final three datasets were selected from results by Probst. There are all three datasets where tuning mtry has a large effect on R2.

```{r}
ds_name <- c("df_friedman1", 
             "df_friedman1_N100", 
             "df_friedman1_N500",
             "df_iris",
             "df_iris_N100",
             "df_NCI60",
             "puma32H",
             "tecator",
             "bodyfat",
             "solubility",
             "solubility_N100",
             "solubility_N500",
             "solubility_N500_perm"
             )

ds_group <- c("df_friedman1", 
             "df_friedman1", 
             "df_friedman1",
             "df_iris",
             "df_iris",
             "df_NCI60",
             "openML",
             "openML",
             "openML",
             "solubility",
             "solubility",
             "solubility",
             "solubility"
             )

ds_target <- c("y",
               "y",
               "y",
               "Species",
               "Species",
               "CLASS",
               "thetadd6",
               "fat",
               "class",
               "y",
               "y",
               "y",
               "y"
             )

ds_forest_type <- c("reg",
               "reg",
               "reg",
               "class",
               "class",
               "class",
               "reg",
               "reg",
               "reg",
               "reg",
               "reg",
               "reg",
               "reg"
             )


if(exists("ds_list")) rm(ds_list)

for(i in 1:length(ds_name)){
  name <- ds_name[i]
  ds_list_tmp <- data.frame(ds_id = i,
                            ds_name = name,
                            ds_group = ds_group[i],
                            target = ds_target[i],
                            forest_type = ds_forest_type[i],
                            n_obs = nrow(get(name)),
                            n_features = ncol(get(name)) - 1
  )
  if(exists("ds_list")){
    ds_list <- rbind(ds_list, ds_list_tmp)
  } else {ds_list <- ds_list_tmp}
}
```

```{r}
knitr::kable(ds_list)
```

# Model list

We want ranger fits for both default and max mtry.

```{r}
model_list <- data.frame(model_id = c(1:3),
                         method = c("ranger", "ranger", "ranger-rfe"), 
                         mtry = c("default", "max", "default"))

model_list
```

`max` is bagging setting, i.e. offer all predictors.

```{r}
fit_list <- crossing(model_list, ds_list)

fit_list$fit_id <- 1:nrow(fit_list)
```

```{r}
fit_list <- fit_list %>% 
  left_join(data.frame(ds_id = 1:13, ds_group)) %>% 
  arrange(ds_group) #%>% filter(ds_group == "df_friedman1")
```

```{r}
fit_list
```

# Fit all methods on all datasets (CV performance)

```{r}
source("rangerFuncs.R")

ctrl <- rfeControl(functions = rangerFuncs,
                   method = "cv",
                   returnResamp = "all",
                   verbose = TRUE,
                   saveDetails = TRUE)
```


```{r}
res <- list()
fit_list$performance <- NA
dir.create("fits")
fullrun <- 0

if(fullrun){
  for (i in 1:nrow(fit_list)){
    print(".")
    
    formula <- as.formula(paste0(fit_list[i,]$target, " ~ ."))
    
    if(fit_list[i,]$method == "ranger"){
        if(fit_list[i,]$mtry == "max"){
          mtry = fit_list[i,]$n_features
        } else {
          mtry = floor(sqrt(fit_list[i,]$n_features))
        }
        set.seed(123)
        fit <- ranger(formula, 
                      data = get(fit_list[i,]$ds_name),
                      mtry = mtry,
                      probability = FALSE, #fit_list[i,]$forest_type == "class",
                      num.threads = 6 )
        
        filename <- paste0("fits/fit_", fit_list[i,]$fit_id, ".rds")
        saveRDS(object = fit, file = filename)
        
        fit_list[i,]$performance <- ifelse(fit_list[i,]$forest_type == "reg", 
                                           fit$r.squared,
                                           1 - fit$prediction.error)
    }
    if(fit_list[i,]$method == "ranger-rfe"){
      
      if(fit_list[i,]$forest_type == "reg"){ 
        perf_metric =  "Rsquared" 
        } else {perf_metric =  "Accuracy"}
 
      subsets <- caret:::var_seq(fit_list[i,]$n_features, len = 10)

      set.seed(123)
      
      rfe_fit <- rfe(formula, 
                 data = get(fit_list[i,]$ds_name),
                 metric = perf_metric,
                 sizes = subsets, 
                 rfeControl = ctrl,
                 num.threads = 6)
        
        filename <- paste0("fits/fit_", fit_list[i,]$fit_id, ".rds")
        saveRDS(object = rfe_fit, file = filename)
        
        fit_list[i,]$performance <- ifelse(fit_list[i,]$forest_type == "reg", 
                                           rfe_fit$fit$r.squared,
                                           1 - rfe_fit$fit$prediction.error)
    }
    
  }
  
  filename <- paste0("fitlist.rds")
  saveRDS(object = fit_list, file = filename)
} else {
  filename <- paste0("fitlist.rds")
  fit_list <- readRDS(filename) 
  fit_list$r_sq <- NULL}

fit_list$algo <- paste0(fit_list$method, "_mtry_", fit_list$mtry)
```



## Ranger separate calls 

### Friedman1 datasets

```{r}
ggplot(fit_list %>% filter(ds_group == "df_friedman1"), 
       aes(x = reorder(algo, performance), y = performance, col = factor(algo))) +
  geom_point() +
  geom_label(aes(label = round(performance,2)), nudge_y = -0.2, nudge_x = +0.1) +
  geom_segment(aes(xend = algo, y = 0, yend = performance)) +
  coord_flip() +
  facet_wrap(~ ds_name)
```

It looks like tuning mtry can reduce the negative effect of irrelevant features, but not completely eliminate it as RFE can do. The subsets are important here: N100 has 16 vars (5 signal, rest noise), N500 has 6 vars.

```{r}
fit_list %>% filter(ds_group == "df_friedman1")
```

```{r}
rfe_fit <- readRDS("fits/fit_29.rds")

rfe_fit$fit
```

```{r}
r.squared <- c()

for(i in 1:30){
  rf.fit <- ranger(y ~ ., data = df_friedman1_N500, num.threads = 6, num.trees = 100)
  r.squared[i] <- rf.fit$r.squared
}
```

```{r}
hist(r.squared)
```
```{r}
r.squared2 <- c()

for(i in 1:30){
  rf.fit <- ranger(y ~ ., data = df_friedman1_N500, num.threads = 6, num.trees = 1000)
  r.squared2[i] <- rf.fit$r.squared
}
```

```{r}
hist(r.squared)
```

```{r}
r.squared3 <- c()

for(i in 1:5){
  rf.fit <- ranger(y ~ ., data = df_friedman1_N500, num.threads = 6, num.trees = 5000)
  r.squared3[i] <- rf.fit$r.squared
}
```

```{r}
hist(r.squared3)
```

### ISLR datasets (NCI60)

```{r}
ggplot(fit_list %>% filter(ds_group == "df_NCI60"), 
       aes(x = reorder(algo, performance), y = performance, col = factor(algo))) +
  geom_point() +
  geom_label(aes(label = round(performance,2)), nudge_y = -0.2, nudge_x = +0.1) +
  geom_segment(aes(xend = algo, y = 0, yend = performance)) +
  coord_flip() +
  facet_wrap(~ ds_name)
```

```{r}
rf <- ranger(CLASS ~ ., probability = FALSE, data = df_NCI60)


table(df_NCI60$CLASS, rf$predictions)

rf
```

Here RFE has not found the "sweet spot", which is weird because the full dataset and default mtry should also be part of the settings.

```{r}
fit_list %>% filter(ds_group == "df_NCI60")

rfe_fit <- readRDS("fits/fit_32.rds")

rfe_fit$fit
```

```{r}
rfe_fit
```


### OpenML datasets

https://topepo.github.io/caret/data-sets.html#tecator-nir-data

For the three OpenML datasets, mtry tuning is sufficient to get optimal performance.

```{r}
ggplot(fit_list %>% filter(ds_group == "openML"), 
       aes(x = reorder(algo, performance), y = performance, col = factor(algo))) +
  geom_point() +
  geom_label(aes(label = round(performance,2)), nudge_y = -0.2, nudge_x = +0.1) +
  geom_segment(aes(xend = algo, y = 0, yend = performance)) +
  coord_flip() +
  facet_wrap(~ ds_name)
```


## caret with mtry tuning (CV / OOB)

### iris datasets

```{r}
ggplot(fit_list %>% filter(ds_group == "df_iris"), 
       aes(x = reorder(algo, performance), y = performance, col = factor(algo))) +
  geom_point() +
  geom_label(aes(label = round(performance,2)), nudge_y = -0.02, nudge_x = +0.1) +
  geom_segment(aes(xend = algo, y = 0, yend = performance)) +
  coord_flip() +
  facet_wrap(~ ds_name)
```


```{r}
set.seed(123)
rangerGrid <- data.frame(mtry = 2:4,
                       splitrule = "gini", 
                       min.node.size = 10)

res <- train(Species ~ ., 
             data = df_iris, 
             method = "ranger", 
             tuneGrid = rangerGrid,            
             trControl = trainControl(method = "cv"))
res
```

```{r}
rangerGrid <- data.frame(mtry = c(2:4,40,80,104),
                       splitrule = "gini", 
                       min.node.size = 10)

res <- train(Species ~ ., 
             data = df_iris_N100, 
             method = "ranger", 
             tuneGrid = rangerGrid,
             trControl = trainControl(method = "cv"),
             num.threads = 6)
res
```

### Cox-2

Note: this is not the same dataset as was used in Svetnik 2003.

From:

Spline-Fitting with a Genetic Algorithm: A Method for Developing Classification
Structure-Activity Relationships
Jeffrey J. Sutherland,† Lee A. O’Brien,‡ and Donald F. Weaver

(i) COX-2. A set of 467 cyclooxygenase-2 (COX-2)
inhibitors has been assembled from the published work of a
single research group, with in vitro activities against human
recombinant enzyme expressed as IC 50 values ranging from
1 nM to >100 μM (53 compounds have indeterminate IC 50
values). A 314 compound subset of these inhibitors has been
studied with QSAR and classification by Kauffman and Jurs.4
They used pIC50 ) 6.5 as the threshold for classifying
compounds as active or inactive, guided by a histogram plot
of compound counts vs pIC 50 . Here, we employ the same
threshold.

```{r}
rangerGrid <- data.frame(mtry = c(2,50,100,255),
                       splitrule = "gini", 
                       min.node.size = 10)

res <- train(class ~ ., 
             data = cox2_class, 
             method = "ranger", 
             tuneGrid = rangerGrid,            
             trControl = trainControl(method = "cv"))
res
```

Predicting always inactive gives us a baseline of 76% accuracy.

### Friedman1 datasets


```{r}
ggplot(fit_list %>% filter(ds_group == "df_friedman1"), aes(x = reorder(algo, performance), y = performance, col = factor(algo))) +
  geom_point() +
  geom_label(aes(label = round(performance,2)), nudge_y = -0.02, nudge_x = +0.1) +
  geom_segment(aes(xend = algo, y = 0, yend = performance)) +
  coord_flip() +
  facet_wrap(~ ds_name)
```

```{r}
set.seed(1234)

rangerGrid <- data.frame(mtry = c(2:5),
                       splitrule = "variance", 
                       min.node.size = 5)

res <- train(y ~ X1 + X2 + X3 + X4 + X5, 
              data = df_friedman1, 
              method = "ranger", 
              tuneGrid = rangerGrid,
              trControl = trainControl(method = "oob")
             )

res 
```

If we tune on the DGP variables, we get the best possible performance.


```{r}
set.seed(1234)

rangerGrid <- data.frame(mtry = c(2:5, 30, 60, 110),
                       splitrule = "variance", 
                       min.node.size = 5)

res <- train(y ~ ., 
              data = df_friedman1_N100, 
              method = "ranger", 
              metric = "Rsquared",
              tuneGrid = rangerGrid,
              trControl = trainControl(method = "oob"),
              num.threads = 6
             )

plot(res)
```

```{r}
res
```
Simply tuning the RF (mtry) does not give optimal performance with 100+5 noise.

```{r}
set.seed(1234)

rangerGrid <- data.frame(mtry = c(5, 60, 110, 160, 220, 510),
                       splitrule = "variance", 
                       min.node.size = 10)

res <- train(y ~ ., 
              data = df_friedman1_N500, 
              method = "ranger", 
              metric = "Rsquared",
              tuneGrid = rangerGrid,
              trControl = trainControl(method = "oob"),
             num.threads = 6
             )

plot(res)
```

This gets worse if we add more noise variables.

```{r}
set.seed(1234)
fullrun <- 1

rangerGrid <- data.frame(mtry = c(60),
                       splitrule = "variance", 
                       min.node.size = 5)

if(fullrun){
res2 <- train(y ~ ., 
              data = df_friedman1_N100, 
              method = "ranger", 
              metric = "Rsquared",
             tuneGrid = rangerGrid,
              trControl = trainControl(method = "repeatedcv", repeats = 10, returnResamp = "all"),
              num.threads = 6,
             num.trees = 250
             )
saveRDS(res2, "res2.rds")
} else { res2 <- readRDS("res2.rds")}
res2
```

Now increase num.trees

```{r}
set.seed(1234)
fullrun <- 1

rangerGrid <- data.frame(mtry = c(60),
                       splitrule = "variance", 
                       min.node.size = 5)

if(fullrun){
res2 <- train(y ~ ., 
              data = df_friedman1_N100, 
              method = "ranger", 
              metric = "Rsquared",
             tuneGrid = rangerGrid,
              trControl = trainControl(method = "repeatedcv", repeats = 10, returnResamp = "all"),
              num.threads = 6,
             num.trees = 2500
             )
saveRDS(res2, "res3.rds")
} else { res2 <- readRDS("res3.rds")}
res2
```

```{r}
sres <- rbind(data.frame(res$resample, num.trees = 500),
              data.frame(res2$resample, num.trees = 5000))

sres$Rep <- substr(sres$Resample, 8,12)

ggplot(sres, aes(x = MAE, y = Rsquared )) +
  facet_wrap(~ Rep)
```


### OpenML datasets

```{r}
ggplot(fit_list %>% filter(ds_group == "openML"), aes(x = reorder(algo, performance), y = performance, col = factor(algo))) +
  geom_point() +
  geom_label(aes(label = round(performance,2)), nudge_y = -0.02, nudge_x = +0.1) +
  geom_segment(aes(xend = algo, y = 0, yend = performance)) +
  coord_flip() +
  facet_wrap(~ ds_name)
```

```{r}
set.seed(1234)

rangerGrid <- data.frame(mtry = c(2,11, 63,124),
                       splitrule = "variance",
                       min.node.size = 5)

res <- train(fat ~ ., 
              data = tecator, 
              method = "ranger", 
              metric = "Rsquared",
              tuneGrid = rangerGrid,
              trControl = trainControl(method = "oob"),
             num.threads = 6
             )

res
```

Here tuning mtry gives us optimal performance.


```{r}
set.seed(1234)

rangerGrid <- data.frame(mtry = c(2,11, 32),
                       splitrule = "variance",
                       min.node.size = 5)

res <- train(thetadd6 ~ ., 
              data = puma32H, 
              method = "ranger", 
              metric = "Rsquared",
              tuneGrid = rangerGrid,
              trControl = trainControl(method = "oob"),
             num.threads = 6
             )

res
```

Here tuning mtry gives us optimal performance.

## rfe - ranger

Common settings:
```{r}
source("rangerFuncs.R")

ctrl <- rfeControl(functions = rangerFuncs,
                   method = "cv",
                   returnResamp = "all",
                   verbose = TRUE,
                   saveDetails = TRUE)
```

### ISLR NCI60


Leave this out of blog, too high variance / uncertainty on the performance metric.

```{r}
ggplot(fit_list %>% filter(ds_group == "df_NCI60"), aes(x = reorder(algo, performance), y = performance, col = factor(algo))) +
  geom_point() +
  geom_label(aes(label = round(performance,2)), nudge_y = -0.02, nudge_x = +0.1) +
  geom_segment(aes(xend = algo, y = 0, yend = performance)) +
  coord_flip() +
  facet_wrap(~ ds_name)
```
low n, high p, genetic marker dataset.

```{r}
#df_NCI60

subsets <- c(10,75,100) # 100,500

set.seed(10)
rfProfile <- rfe(CLASS ~ ., 
                 data = df_NCI60, 
                 metric = "Accuracy",
                 sizes = subsets, 
                 rfeControl = ctrl,
                 num.threads = 6)

rfProfile
```
RFE does not improve. Chooses the model with all predictors.

```{r}
rfProfile$fit
```

The SD is huge here: a rerun gives the highest accuracy to the untuned RF with default mtry.

### Friedman1 datasets

```{r}
subsets <- c(1,4,5,6)

set.seed(10)
rfProfile <- rfe(y ~ ., 
                 data = df_friedman1_N100, 
                 metric = "Rsquared",
                 sizes = subsets, 
                 rfeControl = ctrl,
                 num.threads = 6)

plot(rfProfile)

```
Here, RFE beats mtry tuning. Can we also find a non artificial dataset where this holds?

```{r}
rfProfile
```


### OpenML datasets

```{r}
#subsets <- c(1:10, 150, 200, 300)
subsets <- c(1:5)

set.seed(10)
rfProfile <- rfe(thetadd6 ~ ., 
                 data = puma32H, 
                 metric = "Rsquared",
                 sizes = subsets, 
                 rfeControl = ctrl,
                 num.threads = 6)

plot(rfProfile)
```

```{r}
rfProfile
```


### Solubility models


The APM dataset solubility shows all three methods giving similar results in the absence of noise.
After adding noise, tuning mtry helps, but is not enough.
Again, this is a artificial dataset with a large amount of noise.
The situation gets worse if we add more noise.

As a check that we do not fit on noise, after shuffling the Y variable all three methods produce an R2 of zero.

We can compare this with refitting after the RFE variable selection.

```{r}
ggplot(fit_list %>% filter(ds_group == "solubility"), 
       aes(x = reorder(algo, performance), y = performance, col = factor(algo))) +
  geom_point() +
  geom_label(aes(label = round(performance,2)), nudge_y = -0.05, nudge_x = +0.25) +
  geom_segment(aes(xend = algo, y = 0, yend = performance)) +
  coord_flip() +
  facet_wrap(~ ds_name)
```

### refit models after RFE: this risks selection bias 

```{r}
ranger(fat ~ moisture + protein, 
       data = tecator,
       num.threads = 6)
```

```{r}
ranger(class ~ Density + Abdomen, 
       data = bodyfat,
       num.threads = 6)
```

```{r}
ranger(thetadd6 ~ tau4 + theta5, 
       data = puma32H,
       num.threads = 6)
```

OOB error is identical to RFE/mtry results. Nothing special here.

## rfe - ranger with mtry tuning

Conclusion: although it is technically possible, it has no use case.

```{r}
# source("rangerTuneFuncs.R")
```


## refitting after rfe: solubility permuted


```{r}
source("rangerFuncs.R")

ctrl <- rfeControl(functions = rangerFuncs,
                   method = "cv",
                   returnResamp = "all",
                   verbose = TRUE,
                   saveDetails = TRUE)
```


```{r}
#fit_list %>% filter(method == "ranger-rfe")

#fit <- readRDS("fits/fit_39.rds")

# RFE subsets to use during backward feature selection
set.seed(123)

subsets <- round(exp(seq(log(1), log(ncol(solubility_N500_perm)-1), 
                         length.out = sqrt(ncol(solubility_N500_perm)-1))))

fullrun <- 0

if(fullrun){
  fit <- rfe(y ~., 
             data = solubility_N500_perm,
             sizes = c(35,75,100),
             metric = "RMSE",
             rfeControl = ctrl,
             num.threads = 6)
  
  saveRDS(fit, "rfe_fit.rds")
} else {
  fit <- readRDS("rfe_fit.rds")
}

fit
```


```{r}
# subset on selected features
preds <- predictors(fit)
Xrfe <- solubility_N500_perm %>% select(all_of(preds))

set.seed(123)

# check mtry at different orders of magnitude
tune_grid <- expand.grid(mtry = round(exp(seq(log(1), log(length(preds)), length.out=sqrt(length(preds))))),
                         splitrule = "variance",
                         min.node.size = 5)

fullrun <- 0

if(fullrun) {
  trainObject <- train(x = Xrfe,
             y = solubility_N500_perm$y,
             method = "ranger",
             trControl = trainControl(method = "cv"),
             tuneGrid = tune_grid,
             num.threads = 6)
  saveRDS(trainObject, "post_rfe_train.rds")
} else {
  trainObject <- readRDS("post_rfe_train.rds")
}
```

```{r}
trainObject
```


# TuneRanger

Show that TuneRanger tunes the mtry value.

```{r}
library(tuneRanger)
library(mlr)
library(OpenML)

tecator_data_1 = getOMLDataSet(505)$data
tecator.task = makeRegrTask(data = tecator_data_1, target = "fat")

# Estimate runtime
estimateTimeTuneRanger(tecator.task)
# Approximated time for tuning: 1M 13S
set.seed(123)
# Tuning
res = tuneRanger(tecator.task, num.trees = 500,
num.threads = 6, iters = 70, iters.warmup = 30)
res
```
# References

[@Friedman91] Friedman, Jerome H. (1991) Multivariate adaptive regression splines. The Annals of Statistics 19 (1), pages 1-67.

[@breiman96] Breiman, Leo (1996) Bagging predictors. Machine Learning 24, pages 123-140.

[@Breiman01] Random Forests

[@guyon02] Gene Selection for Cancer Classification using Support Vector Machines

    Isabelle Guyon, Jason Weston, Stephen Barnhill & Vladimir Vapnik 
    
*In this paper, we address the problem of selection of a small subset of genes from broad patterns of gene expression data*
*We propose a new method of gene selection utilizing Support Vector Machine methods based on Recursive Feature Elimination (RFE)*

[@Ambroise02] [Selection bias in gene extraction on the basis of microarray gene-expression data](https://www.pnas.org/content/99/10/6562.short)

[@svetnik03] Svetnik, V, A Liaw, C Tong, C Culberson, R Sheridan, and B Feuston. 2003. 
Random Forest: A Classification and Regression Tool for Compound Classification and QSAR Modeling

*There is no evidence that performance actually improves with descriptor selection. Also, the default mtry has the best performance.*
[...]
*However, it is still important for users to investigate the sensitivity of Random Forest to changes in mtry or descriptor selection, as illustrated in the COX-2 classification data, where mtry = p turned out to be the best choice. For further discussion of descriptor selection with Random Forest, see ref 33.*

*The exceptional case is the COX-2 classification data. In this case, mtry = p (bagging) is the best performer, as can be seen in Figure 3, although descriptor reduction (RFE) only degrades performance.*

[@svetnik04] Application of Breiman’s Random Forest to Modeling Structure-Activity Relationships of Pharmaceutical Molecules

*Without any parameter tuning, the performance of Random Forest with default settings on six publicly available data sets is already as good or better than that of three other prominent QSAR methods: Decision Tree, Partial Least Squares, and Support Vector Machine. In addition to reliable prediction accuracy, Random Forest provides variable importance measures which can be used in a variable reduction wrapper algorithm. (**RFE**)

*We have never yet observed a case where the performance actually improves as variables are reduced*

[@svetnik04b] Svetnik, V.; Liaw, A.; Tong, C. Variable selection in random forest with application to quantitative structure-activity relationship. In Proceedings of the 7th Course on Ensemble Methods for Learning Machines; Intrator, N., Masulli, F., Eds.; Springer-Verlag: Berlin, 2004; submitted

*It is shown that the non-recursive version of the procedure outperforms the recursive version, and that the default Random Forest mtry function is usually adequate.*

[@geurts06] Extremely randomized trees

[@genuer_variable_2010] variable selection using random forests

*The main contribution is twofold: to provide some insights about the behavior of the variable importance index based on random forests and to propose a strategy involving a ranking of explanatory variables using the random forests score of importance and a stepwise ascending variable introduction strategy*
<!-- four classification, one regression, Ozone -->

[@kuhn13] Applied Predictive Modelling, with (mostly empty) Tidymodels companion at https://github.com/topepo/tidy-apm

[@Kuhn19] Feature Engineering and Selection: A Practical Approach for Predictive Models, online at 
http://www.feat.engineering/

[@Couronne18] Random forest versus logistic regression: a large-scale benchmark experiment (also with Probst)

[@Probst19] Hyperparameters and Tuning Strategies for Random Forest, Arxiv

[@Demircioglu21] Measuring the bias of incorrect application of feature selection when using cross-validation in radiomics



