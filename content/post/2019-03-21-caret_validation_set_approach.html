---
title: The validation set approach in caret
author: Gertjan Verhoeven
date: '2019-03-21'
summary: In this blog post, we explore how to implement the validation set approach in caret. This is the most basic form of the train/test machine learning concept.
draft: FALSE
slug: validation-set-approach-in-caret
categories:
  - R
tags:
  - R, caret 
baseurl: "https://gsverhoeven.github.io"
header:
  image: "headers/balls-blur-close-up-278911.jpg"
  preview: FALSE

---



<p>In this blog post, we explore how to implement the <em>validation set approach</em> in <code>caret</code>. This is the most basic form of the train/test machine learning concept. For example, the classic machine learning textbook <a href="http://www-bcf.usc.edu/~gareth/ISL/">“An introduction to Statistical Learning”</a> uses the validation set approach to introduce resampling methods.</p>
<p>In practice, one likes to use k-fold Cross validation, or Leave-one-out cross validation, as they make better use of the data. This is probably the reason that the validation set approach is not one of <code>caret</code>’s preset methods.</p>
<p>But for teaching purposes it would be very nice to have a <code>caret</code> implementation.</p>
<p>This would allow for an easy demonstration of the variability one gets when choosing different partionings. It also allows direct demonstration of why k-fold CV is superior to the validation set approach with respect to bias/variance.</p>
<p>We pick the <code>BostonHousing</code> dataset for our example code.</p>
<pre class="r"><code># Boston Housing 
knitr::kable(head(Boston))</code></pre>
<table>
<thead>
<tr class="header">
<th align="right">crim</th>
<th align="right">zn</th>
<th align="right">indus</th>
<th align="right">chas</th>
<th align="right">nox</th>
<th align="right">rm</th>
<th align="right">age</th>
<th align="right">dis</th>
<th align="right">rad</th>
<th align="right">tax</th>
<th align="right">ptratio</th>
<th align="right">black</th>
<th align="right">lstat</th>
<th align="right">medv</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">0.00632</td>
<td align="right">18</td>
<td align="right">2.31</td>
<td align="right">0</td>
<td align="right">0.538</td>
<td align="right">6.575</td>
<td align="right">65.2</td>
<td align="right">4.0900</td>
<td align="right">1</td>
<td align="right">296</td>
<td align="right">15.3</td>
<td align="right">396.90</td>
<td align="right">4.98</td>
<td align="right">24.0</td>
</tr>
<tr class="even">
<td align="right">0.02731</td>
<td align="right">0</td>
<td align="right">7.07</td>
<td align="right">0</td>
<td align="right">0.469</td>
<td align="right">6.421</td>
<td align="right">78.9</td>
<td align="right">4.9671</td>
<td align="right">2</td>
<td align="right">242</td>
<td align="right">17.8</td>
<td align="right">396.90</td>
<td align="right">9.14</td>
<td align="right">21.6</td>
</tr>
<tr class="odd">
<td align="right">0.02729</td>
<td align="right">0</td>
<td align="right">7.07</td>
<td align="right">0</td>
<td align="right">0.469</td>
<td align="right">7.185</td>
<td align="right">61.1</td>
<td align="right">4.9671</td>
<td align="right">2</td>
<td align="right">242</td>
<td align="right">17.8</td>
<td align="right">392.83</td>
<td align="right">4.03</td>
<td align="right">34.7</td>
</tr>
<tr class="even">
<td align="right">0.03237</td>
<td align="right">0</td>
<td align="right">2.18</td>
<td align="right">0</td>
<td align="right">0.458</td>
<td align="right">6.998</td>
<td align="right">45.8</td>
<td align="right">6.0622</td>
<td align="right">3</td>
<td align="right">222</td>
<td align="right">18.7</td>
<td align="right">394.63</td>
<td align="right">2.94</td>
<td align="right">33.4</td>
</tr>
<tr class="odd">
<td align="right">0.06905</td>
<td align="right">0</td>
<td align="right">2.18</td>
<td align="right">0</td>
<td align="right">0.458</td>
<td align="right">7.147</td>
<td align="right">54.2</td>
<td align="right">6.0622</td>
<td align="right">3</td>
<td align="right">222</td>
<td align="right">18.7</td>
<td align="right">396.90</td>
<td align="right">5.33</td>
<td align="right">36.2</td>
</tr>
<tr class="even">
<td align="right">0.02985</td>
<td align="right">0</td>
<td align="right">2.18</td>
<td align="right">0</td>
<td align="right">0.458</td>
<td align="right">6.430</td>
<td align="right">58.7</td>
<td align="right">6.0622</td>
<td align="right">3</td>
<td align="right">222</td>
<td align="right">18.7</td>
<td align="right">394.12</td>
<td align="right">5.21</td>
<td align="right">28.7</td>
</tr>
</tbody>
</table>
<p>Our model is predicting <code>medv</code> (Median house value) using predictors <code>indus</code> and <code>chas</code> in a multiple linear regression. We split the data in half, 50% for fitting the model, and 50% to use as a validation set.</p>
<div id="stratified-sampling-vs-random-sampling" class="section level1">
<h1>Stratified sampling vs random sampling</h1>
<p>To check if we understand what <code>caret</code> does, we first implement the validation set approach ourselves. To be able to compare, we need exactly the same data partitions for our manual approach and the <code>caret</code> approach. As <code>caret</code> requires a particular format (a named list of sets of train indices) we conform to this standard. However, all <code>caret</code> partitioning functions seem to perform <strong>stratified random sampling</strong>. This means that it first partitions the data in equal sized groups based on the outcome variable, and then samples at random <strong>within those groups</strong> to partitions that have similar distributions for the outcome variable.</p>
<p>This not desirable for teaching, as it adds more complexity. In addition, it would be nice to be able to compare stratified vs. random sampling.</p>
<p>We therefore write a function that generates truly random partitions of the data. We let it generate partitions in the format that <code>trainControl</code> likes.</p>
<pre class="r"><code># internal function from caret package, needed to play nice with resamples()
prettySeq &lt;- function(x) paste(&quot;Resample&quot;, gsub(&quot; &quot;, &quot;0&quot;, format(seq(along = x))), sep = &quot;&quot;)

createRandomDataPartition &lt;- function(y, times, p) {
  vec &lt;- 1:length(y)
  n_samples &lt;- round(p * length(y))
  
  result &lt;- list()
  for(t in 1:times){
    indices &lt;- sample(vec, n_samples, replace = FALSE)
    result[[t]] &lt;- indices
    #names(result)[t] &lt;- paste0(&quot;Resample&quot;, t)
  }
  names(result) &lt;- prettySeq(result)
  result
}

createRandomDataPartition(1:10, times = 2, p = 0.5)</code></pre>
<pre><code>## $Resample1
## [1] 7 2 4 5 6
## 
## $Resample2
## [1] 4 6 5 3 9</code></pre>
</div>
<div id="the-validation-set-approach-without-caret" class="section level1">
<h1>The validation set approach without caret</h1>
<p>Here is the validation set approach without using caret. We create a single random partition of the data in train and validation set, fit the model on the training data, predict on the validation data, and calculate the RMSE error on the test predictions.</p>
<pre class="r"><code>set.seed(1234)
parts &lt;- createRandomDataPartition(Boston$medv, times = 1, p = 0.5)

train &lt;- parts$Resample1

# fit ols on train data
lm.fit &lt;- lm(medv ~ indus + chas , data = Boston[train,])

# predict on held out data
preds &lt;- predict(lm.fit, newdata = Boston[-train,])

# calculate RMSE validation error
sqrt(mean((preds - Boston[-train,]$medv)^2))</code></pre>
<pre><code>## [1] 8.217625</code></pre>
<p>If we feed <code>caret</code> the same data partition, we expect <em>exactly</em> the same test error for the held-out data. Let’s find out!</p>
</div>
<div id="the-validation-set-approach-in-caret" class="section level1">
<h1>The validation set approach in caret</h1>
<p>Now we use the <code>caret</code> package. Regular usage requires two function calls, one to <code>trainControl</code> to control the resampling behavior, and one to <code>train</code> to do the actual model fitting and prediction generation.</p>
<p>As the validation set approach is not one of the predefined methods, we need to make use of the <code>index</code> argument to explicitely define the train partitions outside of <code>caret</code>. It automatically predicts on the records that are not contained in the train partitions.</p>
<p>The <code>index</code> argument plays well with the <code>createDataPartition</code> (Stratfied sampling) and <code>createRandomDataPartition</code> (our own custom function that performs truly random sampling) functions, as these functions both generate partitions in precisely the format that <code>index</code> wants: lists of training set indices.</p>
<p>In the code below, we generate four different 50/50 partitions of the data.</p>
<p>We set <code>savePredictions</code> to <code>TRUE</code> to be able to verify the calculated metrics such as the test RMSE.</p>
<pre class="r"><code>set.seed(1234)

# create four partitions
parts &lt;- createRandomDataPartition(Boston$medv, times = 4, p = 0.5)

ctrl &lt;- trainControl(method = &quot;repeatedcv&quot;, 
                     ## The method doesn&#39;t matter
                     ## since we are defining the resamples
                     index= parts, 
                     ##verboseIter = TRUE, 
                     ##repeats = 1,
                     savePredictions = TRUE
                     ##returnResamp = &quot;final&quot;
                     ) </code></pre>
<p>Now we can run caret and fit the model four times:</p>
<pre class="r"><code>res &lt;- train(medv ~ indus + chas, data = Boston, method = &quot;lm&quot;,
             trControl = ctrl)

res</code></pre>
<pre><code>## Linear Regression 
## 
## 506 samples
##   2 predictor
## 
## No pre-processing
## Resampling: Cross-Validated (10 fold, repeated 1 times) 
## Summary of sample sizes: 253, 253, 253, 253 
## Resampling results:
## 
##   RMSE      Rsquared  MAE     
##   8.140461  0.221556  5.970956
## 
## Tuning parameter &#39;intercept&#39; was held constant at a value of TRUE</code></pre>
<p>From the result returned by <code>train</code> we can verify that it has fitted a model on four different datasets, each of size <code>253</code>. By default it reports the average test error over the four validation sets. We can also extract the four individual test errors:</p>
<pre class="r"><code># strangely enough, resamples() always wants at least two train() results
# see also the man page for resamples()
resamples &lt;- resamples(list(MOD1 = res, 
                            MOD2 = res))

resamples$values$`MOD1~RMSE`</code></pre>
<pre><code>## [1] 8.217625 7.960800 8.244937 8.138481</code></pre>
<pre class="r"><code># check that we recover the RMSE reported by train() in the Resampling results
mean(resamples$values$`MOD1~RMSE`)</code></pre>
<pre><code>## [1] 8.140461</code></pre>
<pre class="r"><code>summary(resamples)</code></pre>
<pre><code>## 
## Call:
## summary.resamples(object = resamples)
## 
## Models: MOD1, MOD2 
## Number of resamples: 4 
## 
## MAE 
##          Min.  1st Qu.   Median     Mean  3rd Qu.     Max. NA&#39;s
## MOD1 5.772957 5.949938 6.011035 5.970956 6.032053 6.088796    0
## MOD2 5.772957 5.949938 6.011035 5.970956 6.032053 6.088796    0
## 
## RMSE 
##        Min.  1st Qu.   Median     Mean  3rd Qu.     Max. NA&#39;s
## MOD1 7.9608 8.094061 8.178053 8.140461 8.224453 8.244937    0
## MOD2 7.9608 8.094061 8.178053 8.140461 8.224453 8.244937    0
## 
## Rsquared 
##           Min.   1st Qu.    Median     Mean   3rd Qu.      Max. NA&#39;s
## MOD1 0.2017943 0.2069602 0.2189169 0.221556 0.2335127 0.2465958    0
## MOD2 0.2017943 0.2069602 0.2189169 0.221556 0.2335127 0.2465958    0</code></pre>
<p>Note that the RMSE value for the first train/test partition is exactly equal to our own implementation of the validation set approach. Awesome.</p>
</div>
<div id="validation-set-approach-stratified-sampling-versus-random-sampling" class="section level1">
<h1>Validation set approach: stratified sampling versus random sampling</h1>
<p>Since we now know what we are doing, let’s perform a simulation study to compare stratified random sampling with truly random sampling, using the validation set approach, and repeating this proces say a few thousand times to get a nice distribution of test errors.</p>
<pre class="r"><code># simulation settings
n_repeats &lt;- 3000
train_fraction &lt;- 0.8</code></pre>
<p>First we fit the models on the random sampling data partitions:</p>
<pre class="r"><code>set.seed(1234)
parts &lt;- createRandomDataPartition(Boston$medv, times = n_repeats, p = train_fraction)

ctrl &lt;- trainControl(method = &quot;repeatedcv&quot;,  ## The method doesn&#39;t matter
                     index= parts, 
                     savePredictions = TRUE
                     ) 

rand_sampl_res &lt;- train(medv ~ indus + chas, data = Boston, method = &quot;lm&quot;,
             trControl = ctrl)

rand_sampl_res</code></pre>
<pre><code>## Linear Regression 
## 
## 506 samples
##   2 predictor
## 
## No pre-processing
## Resampling: Cross-Validated (10 fold, repeated 1 times) 
## Summary of sample sizes: 405, 405, 405, 405, 405, 405, ... 
## Resampling results:
## 
##   RMSE      Rsquared   MAE     
##   7.872387  0.2755166  5.806044
## 
## Tuning parameter &#39;intercept&#39; was held constant at a value of TRUE</code></pre>
<p>Next, we fit the models on the stratified sampling data partitions:</p>
<pre class="r"><code>set.seed(1234)
parts &lt;- createDataPartition(Boston$medv, times = n_repeats, p = train_fraction, list = T)

ctrl &lt;- trainControl(method = &quot;repeatedcv&quot;,  ## The method doesn&#39;t matter
                     index= parts, 
                     savePredictions = TRUE
                     ) 

strat_sampl_res &lt;- train(medv ~ indus + chas, data = Boston, method = &quot;lm&quot;,
             trControl = ctrl)

strat_sampl_res</code></pre>
<pre><code>## Linear Regression 
## 
## 506 samples
##   2 predictor
## 
## No pre-processing
## Resampling: Cross-Validated (10 fold, repeated 1 times) 
## Summary of sample sizes: 407, 407, 407, 407, 407, 407, ... 
## Resampling results:
## 
##   RMSE      Rsquared  MAE     
##   7.818498  0.280648  5.759168
## 
## Tuning parameter &#39;intercept&#39; was held constant at a value of TRUE</code></pre>
<p>Then, we merge the two results to compare the distributions:</p>
<pre class="r"><code>resamples &lt;- resamples(list(RAND = rand_sampl_res, 
                          STRAT = strat_sampl_res))</code></pre>
</div>
<div id="analyzing-caret-resampling-results" class="section level1">
<h1>Analyzing caret resampling results</h1>
<p>We now analyse our resampling results. We can use the <code>summary</code> method on our resamples object:</p>
<pre class="r"><code>summary(resamples)</code></pre>
<pre><code>## 
## Call:
## summary.resamples(object = resamples)
## 
## Models: RAND, STRAT 
## Number of resamples: 3000 
## 
## MAE 
##           Min.  1st Qu.   Median     Mean  3rd Qu.     Max. NA&#39;s
## RAND  4.100652 5.492730 5.790283 5.806044 6.104647 7.663345    0
## STRAT 4.438645 5.464873 5.745391 5.759168 6.044985 7.493055    0
## 
## RMSE 
##           Min.  1st Qu.   Median     Mean  3rd Qu.     Max. NA&#39;s
## RAND  5.119472 7.331857 7.857935 7.872387 8.403391 11.17930    0
## STRAT 5.634478 7.307348 7.815735 7.818498 8.314292 10.43092    0
## 
## Rsquared 
##             Min.   1st Qu.    Median      Mean   3rd Qu.      Max. NA&#39;s
## RAND  0.05006335 0.2244762 0.2743783 0.2755166 0.3260239 0.5129500    0
## STRAT 0.08044329 0.2311793 0.2792260 0.2806480 0.3286326 0.5242075    0</code></pre>
<p>We can also use the plot function provided by the <code>caret</code> package. It plots the mean of our performance metric (RMSE), as well as estimation uncertainty of this mean. Note that the confidence intervals here are based on a normal approximation (One sample t-test).</p>
<pre class="r"><code># caret:::ggplot.resamples
# t.test(resamples$values$`RAND~RMSE`)
ggplot(resamples, metric = &quot;RMSE&quot;) </code></pre>
<p><img src="/post/2019-03-21-caret_validation_set_approach_files/figure-html/unnamed-chunk-12-1.png" width="672" /></p>
<p>My personal preference is to more directly display both distributions. This is done by <code>bwplot()</code> (<code>caret</code> does not have ggplot version of this function).</p>
<pre class="r"><code>bwplot(resamples, metric = &quot;RMSE&quot;)</code></pre>
<p><img src="/post/2019-03-21-caret_validation_set_approach_files/figure-html/unnamed-chunk-13-1.png" width="672" /></p>
<p>It does seems that stratified sampling paints a slightly more optimistic picture of the test error when compared to truly random sampling. However, we can also see that random sampling has somewhat higher variance when compared to stratified sampling.</p>
<p>Based on these results, it seems like stratified sampling is indeed a reasonable default setting for <code>caret</code>.</p>
</div>
<div id="update-lgocv" class="section level1">
<h1>Update: LGOCV</h1>
<pre class="r"><code>set.seed(1234)


ctrl &lt;- trainControl(method = &quot;LGOCV&quot;,  ## The method doesn&#39;t matter
                     repeats = n_repeats,
                     number = 1,
                     p = 0.5,
                     savePredictions = TRUE
                     ) </code></pre>
<pre><code>## Warning: `repeats` has no meaning for this resampling method.</code></pre>
<pre class="r"><code>lgocv_res &lt;- train(medv ~ indus + chas, data = Boston, method = &quot;lm&quot;,
             trControl = ctrl)

lgocv_res</code></pre>
<pre><code>## Linear Regression 
## 
## 506 samples
##   2 predictor
## 
## No pre-processing
## Resampling: Repeated Train/Test Splits Estimated (1 reps, 50%) 
## Summary of sample sizes: 254 
## Resampling results:
## 
##   RMSE     Rsquared   MAE     
##   7.73795  0.2942766  5.606969
## 
## Tuning parameter &#39;intercept&#39; was held constant at a value of TRUE</code></pre>
</div>
