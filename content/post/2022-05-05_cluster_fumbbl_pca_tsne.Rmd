---
title: 'Cluster analysis of Blood Bowl teams using FUMBBL data in R: PCA vs T-SNE'
author: "Gertjan Verhoeven"
date: '2022-05-05'
summary: This blog post compares two unsupervised learning methods to learn which Blood Bowl teams are similar. Similarity is quantified by looking atvarious match related metrics from such as scoring, injuries, passes and blocks.
slug: blood-bowl-cluster-analysis
draft: yes
categories:
- Blood Bowl
- Machine Learning
- Clustering
- Unsupervised learning
tags:
- t-sne
- pca
baseurl: https://gsverhoeven.github.io
header:
  image: headers/wilhelm-gunkel-di8ognBauG0-unsplash.png
  preview: no
---

This is my third blog post on Blood Bowl, the game of Fantasy Football. Blood Bowl can be summarized as "chess-with-dice" , but this would hardly do the game justice. For example, in chess both players play with the same pieces, but in Blood Bowl, almost 30 different teams (e.g. orcs, elves, etc) are available to choose from, each team with different skills that require different playing styles. In addition, Blood Bowl coaches must assemble and paint their playing pieces themselves, making it a creative hobby as well.

Using data analysis, we can compare teams on their performance statistics and see which teams cluster together. Teams that cluster together have a similar set of performance statistics. Teams can of course still be different on some aspect we did not include in the analysis, but let's just have a look at what we get! 

This post at its heart is basically an update from [this 2018 nufflytics blog post](https://www.nufflytics.com/post/bash-dash-hybrid-by-the-numbers/).
In that blog post, the author used cluster analysis to categorize (classify) the different teams in six clusters, depending on their play style as well as performance. Since then, there have been changes to the rules, and new teams have been created, such as Imperial Nobility, Khorne, Black Orcs etc. It will be interesting to see where these teams fit in.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
# Load packages
library(tidyverse)
library(ggfortify)
library(ggrepel)
```

We starting with reading in the scraped FUMBBL match data.
From the full dataset, we select four groups of Blood Bowl matches to compare: Three divisions (Blackbox, Ranked and League) that used 2016 rulesets and were active up until september 2020, and the new Competitive division that uses the ruleset from the new Blood Bowl 2020 version of the game.

```{r}
df_mbt <- read.csv(file = "../../../fumbbl_datasets/datasets/current/df_mbt.csv")

race_types <- unique(df_mbt %>% select(race_name, race_type))

divisions <- c("Competitive", "Blackbox", "Ranked", "League")

filter_division <- function(div_name){
  df_mbt %>% 
  filter(division_name == div_name) %>%
  filter(race_name != "Treeman") %>%
  filter(race_name != "Simyin")
}

data_tables <- map(divisions, filter_division)
names(data_tables) <- divisions

select_stats <- function(df) {
  df %>% 
  select(race_name, race_type, team_score, away_team_score:away_cas_rip) %>%
  select(-(home_cas_bh:home_cas_rip), -(away_cas_bh:away_cas_rip))
}

data_tables <- map(data_tables, select_stats)

data_tables$Competitive %>% colnames()
```

# Comparing the races on a single statistic

Suppose we want to compare the 25+ Blood Bowl races on their match statistics with each other, and see which races have comparable stats.
If we have only one statistic, it is easy: we can just plot the races on a horizontal line and see which races are closest by.

Lets do that for the **number of blocks made**:

```{r}
res <- data_tables$Competitive %>% group_by(race_name, race_type) %>% summarise(avg_block = mean(home_block), size = n())
ggplot(res, aes(x = avg_block, y = 0, size = size, col = race_type)) +
         geom_point() +
scale_size_area()
```

```{r}
df_mbt %>% filter(home_comp == 0 & race_name == "Wood Elf" & division_name == "Competitive")
```

But races can be similar if we look at **blocks**, but dissimilar if we look at e.g. **passing**. So lets add passing on the y axis.
It should be noted that the passing statistic can also be negative, so races that pass "back" in the direction of their own touch down line can make many passes but still end up with a low average passing distance. 
Here we look at average succesfull passing completions.

```{r}
res <- data_tables$Competitive %>% 
  group_by(race_name, race_type) %>% 
  summarise(avg_block = mean(home_block), 
            avg_pass = mean(abs(home_comp)), 
            size = n())

ggplot(res, aes(x = avg_block, y = avg_pass, size = size, col = race_type)) +
         geom_point() +
  scale_size_area() +
  expand_limits(y=-0.5) +
  geom_label_repel(aes(label = race_name), size  = 2)
  

```
If we look at the **Human** team race,  we can  **Imperial Nobility** and **Old World Alliance**, than it is to **Skaven**. That
Although Human has the same average passing distance as Skaven (which is surprisingly short, as there are a lot of 0 distance passes), the number of blocks for Skaven is much lower, and thus if we compare on both statistics, Imperial Nobility and OWA win out.

AndyDavo Chaos are a bash team, but Undead is hybrid. Zero agility access. 

Now with two statistics, we can simply plot them and calculate a distance between all points.
What if we have more than two statistics? E.g. we have 5 statistics? We have too many dimensions to plot.
For these there exists so-called dimension reduction techniques.
In this blog post we'll examine two of them.

So we have for each combination of race and performance statistic (touchdowns, blocks etc) a value.
We can now either try to group the performance variables, this is about correlations between variables. Which variables are related to each other? 
This produces things like a correlation network graph. This is a poor mans factor analysis.
Alternatively, we can try to group the races: which races are close by? This is cluster analysis.
How can this be different whereas we simply can flip the dataset and have races as columns and performance stats as rows?

# Selecting variables to include

The nufflytics blog post has a somewhat surprising selection of match statistics.
For example, the number of inflicted touchdowns is selected, but not the number of sustained touchdowns.

# Blood Bowl 2 performance statistics

The Cyanide Blood Bowl 2 API (not publicly accessible) apparently offers the following stats for each match, for both teams:

Scoring related:

* Ball occupation (not available in FUMBBL)
* Touchdowns
* Meters running (rushes)

Blocking related:

* Casualties
* Knock outs
* Injuries
* Dead

Passing related:

* Interceptions (not available in FUMBBL)
* Meters passing (passing yards)
* Passes (completions)

Fouling related:

* Expulsions (sent off players after fouling)
* FUMBBL: fouls (not available in BB2 API)

Other:

* Tackles

# Simulated data

He mentions that each match generates two data points, one for each team.

If we include all stats for both the home and away team, something peculiar happens:
The principal components are exactly opposite.

```{r}
set.seed(123)
home_score <- rpois(n = 10000, lambda = 1)
away_score <- rpois(n = 10000, lambda = 1)

df <- rbind(data.frame(home_score, away_score), 
            data.frame(home_score = away_score, away_score = home_score))

PCA <-  df %>% 
    as.matrix() %>% 
    prcomp(scale = T)
PCA
```

```{r}
round(PCA$sdev^2/sum(PCA$sdev^2)*100,1)
```

```{r}
autoplot(PCA, loadings = TRUE, loadings.label = TRUE)
```
Can we fix this by selecting at random one team for each match?

```{r}
set.seed(123)
sel_vec <- rbinom(n = 10000, size = 1, p = 0.5)

sel_vec <- c(sel_vec, 1 -  sel_vec)


```


```{r}
PCA <- df[sel_vec == 1,] %>% 
    as.matrix() %>% 
    prcomp(scale = T)
PCA
```
```{r fig.height = 4, fig.width = 4}
autoplot(PCA, loadings = TRUE, loadings.label = TRUE)
```

```{r}
cor(df$home_score, df$away_score)
```
The scores are completely uncorrelated, even with the duplicate "mirror" matches included.

# Select for each match only one team at random

```{r}
set.seed(123)
sel_vec <- rbinom(n = nrow(df_mbt)/2, size = 1, p = 0.5)

sel_vec <- c(sel_vec, 1 -  sel_vec)

  
# check if it works
df_mbt2 <- df_mbt[sel_vec == 1,]
df_mbt2 <- df_mbt2 %>% filter(match_id != 4239544)
```

We leave out the cas breakdown in badly hurt, injured and dead, since this biases the analysis strongly to differences in Armor value.

# Analysis

```{r}
run_pca <- function(df) {
  df %>% 
    select(-race_name, -race_type) %>% 
    as.matrix() %>% 
    prcomp(scale = T)
}

PCAs <- map(data_tables, run_pca)
```

```{r}
PCAs[[1]]
```

```{r}
autoplot(PCAs[1], loadings = TRUE, loadings.label = TRUE)
```

```{r}
round(PCA$sdev^2/sum(PCA$sdev^2)*100,1)
```
The first two components explain 33% of variation in the match statistics.
The more variables we include, the lower the first components explain variation.

```{r}
cbind(as_data_frame(PCAs[[1]]$x), race = data_tables[[1]]$race_name) %>% 
  ggplot(aes(x = PC1, y = -1*PC2, colour = factor(race) %>% forcats::fct_reorder2(PC1, PC2, sum))) +
  geom_point(alpha = 0.5) +
  theme(legend.position = "none") +
  ggtitle("PCA", "FUMBBL")
```
```{r fig.width= 16, fig.height = 16}
plot_pca <- function(pca, data, flip_x = 1, flip_y = 1, league) { 
  cbind(data.frame(pca$x), race = data$race_name, race_type = data$race_type) %>% 
    group_by(race) %>% 
    summarise(PC1 = median(PC1)*flip_x, 
              PC2 = median(PC2)*flip_y, 
              count = n()) %>% 
    left_join(race_types, by = c("race" = "race_name")) %>%
    ggplot(aes(PC1, PC2, label = race, colour = factor(race_type))) +
      geom_point(aes(size = count), alpha = 0.4) +
      ggrepel::geom_text_repel(size = 4, point.padding = unit(0.05, "lines")) + 
      theme(legend.position = "none") +
      ggtitle(league) +
      scale_x_continuous(expand = c(0.2,0))+
      scale_y_continuous(expand = c(0.2,0))
}

# do a parallel map over multiple inputs (lists)
plots = pmap(.l = list(PCAs, data_tables, 
                       flip_x = c(-1,-1,1,-1), 
                       flip_y = c(1,1,-1,1), 
                       names(league_data)), 
             .f = plot_pca)

cowplot::plot_grid(plotlist = plots)
```

# back to the data

```{r}
res <- data_tables$Competitive %>%
  mutate(pc1 = PCAs[[1]]$x[,1]) %>%
  group_by(race_name) %>%
  select(where(is.numeric)) %>%
  summarise(across(everything(), mean)) %>%
  arrange(-home_block)

View(res)
```



# T-SNE

Swiss Roll (Geron book)
https://jlmelville.github.io/smallvis/swisssne.html

https://members.loria.fr/moberger/Enseignement/AVR/Exposes/TR_Dimensiereductie.pdf

Jesse Krijthe created an R package that "wraps" around the C++ code of Laurens van der Maaten.
t-distributed Stochastic Nearest Neighbour Embedding (T-SNE) needs some serious computation that scales unfavourably with the size of the dataset. There are various tricks to reduce computation. A Tree model is built to find 3 * perplexity nearest neighbours, ignoring the rest. The Barnes-Hutt approximation is controlled by the $theta$ parameter. To help the algorithm an initial PCA step is also computed.

A high perplexity means that a lot of nearest neighbours are taken into account. This means that the higher this parameter, the more the clustering resembles the global structure, i.e. how ALL datapoints relate to each other.

```{r}
library(Rtsne)

set.seed(1234)

## 15 min

TSNE <-  data_tables$Blackbox %>% 
    select(-race_name, -race_type) %>% 
    as.matrix() %>% 
    Rtsne(check_duplicates = FALSE, pca = TRUE, perplexity=50, max_iter = 1000, theta=0.5, dims=2, verbose = TRUE, num_threads = 2 )
```

```{r fig.height = 5, fig.width = 7}
flip_x <- -1
flip_y <- 1

cbind(data.frame(TSNE$Y), race = df_pca$race_name, race_type = df_pca$race_type) %>% 
    group_by(race) %>% 
    summarise(PC1 = median(X1)*flip_x, PC2 = median(X2)*flip_y, count = n()) %>% 
    left_join(race_types, by = c("race" = "race_name")) %>%
     ggplot(aes(PC1, PC2, label = race, colour = factor(race_type))) +
    geom_point(aes(size = count), alpha = 0.4) +
    ggrepel::geom_text_repel(size = 3, point.padding = unit(0.05, "lines")) + 
    #theme(legend.position = "none") +
    ggtitle("FUMBBL t-sne clustering") +
    scale_x_continuous(expand = c(0.2,0)) +
    scale_y_continuous(expand = c(0.2,0))
```
Interestingly, the non-linear clustering has placed Black Orc close to Lizardmen, which sort of makes sense given the similarities between both teams. The big difference is in fouling, and that is only one characteristic of the 14 included.

Khorne is close to humans and Chaos Renegade.

This suggests that the new teams are not typed properly by me.
Ogre should be stunty as well.

